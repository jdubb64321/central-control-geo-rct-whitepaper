COMPLIANCE MONITORING,,,
Track these metrics weekly:,,,
METRIC,ON TARGET,YELLOW FLAG,RED FLAG
% TREATMENT DMAS COMPLIANT,95%+,90-94%,<90%
% CONTROL DMAS WITH SPILLOVER,<5%,5-10%,>10%
SPEND VARIANCE ACROSS TREATMENT,<20%,20-30%,>30%
PLATFORM DELIVERY DISCREPANCIES,<10%,10-20%,>20%
,"(For sample code, s",,
For Syndicated Data:,Primary Analysis –,,
Request data pull with these specifications:,,,
• Markets: All 210 US DMAs (or subset used,INTERPRETING R,,
in test),The coefficient on a,,
• Metrics: [Total sales volume / unit sales /,represents the esti,,
transactions],,,
• Time period: [Full date range including pre and,Estimate: 0.0,,
post periods],,,
,Std. Error: 0,,
"• Granularity: Daily (preferred, more statistical",,,
power) or weekly,t value: 2.19,,
Allow 5-7 business days after period close for,p-value: 0.02,,
"transaction settlement and data processing, then",95% CI: [0.00,,
freeze the dataset for analysis.,,,
,This indicates a 3.4,,
DATA QUALITY CHECKS,significant at the 95,,
"If using sales volume (continuous), check",,,
distribution of normalized values.,,,
"For count data, standard t-test typically suffices",,,
"unless counts are very low (< 5 per DMA-week), in",,,
which case consider Poisson regression.,,,
in test),The coefficient on assignment Treatment,,
• Metrics: [Total sales volume / unit sales /,represents the estimated lift. For example:,,
transactions],,,
• Time period: [Full date range including pre and,Estimate: 0.0342,,
post periods],,,
,Std. Error: 0.0156,,
"• Granularity: Daily (preferred, more statistical",,,
power) or weekly,t value: 2.19,,
Allow 5-7 business days after period close for,p-value: 0.029,,
"transaction settlement and data processing, then","95% CI: [0.0036, 0.0648]",,
freeze the dataset for analysis.,,,
,"This indicates a 3.42% lift (p = 0.029), statistically",,
DATA QUALITY CHECKS,significant at the 95% confidence level.,,
"If using sales volume (continuous), check",,,
distribution of normalized values.,,,
"For count data, standard t-test typically suffices",,,
"unless counts are very low (< 5 per DMA-week), in",,,
which case consider Poisson regression.,,,
DIFFERENCE-IN-DIFFERENCES 0.97,-,CONFIRM,
,,COMPARI,
PRE-PERIOD BALANCE CHECK 1.0,1,NO SIGNI,
(T-TEST),,GROUPS,
LEAVE-ONE-OUT (MEAN EFFECT) 3.61,,TREATME,
,,NO SINGL,
DESCRIPTION OF CHECKS,,,
1. Difference-in-Differences (DiD),,,
Compares the change in outcome between Treatment and Control groups f,,,
This provides a cross-check that the estimated lift is not merely due to diff,,,
over time.,,,
2. Pre-Period Balance Check,,,
A simple t-test comparing average pre-treatment outcomes between Treat,,,
"A high p-value (e.g., >0.1) suggests the groups were balanced before treat",,,
of confounding.,,,
3. Leave-One-Out Sensitivity,,,
"Runs the treatment effect estimation multiple times, each time excluding o",,,
(T-TEST),GROUPS WERE WELL BALANCED AT BASELINE.,,
LEAVE-ONE-OUT (MEAN EFFECT) 3.61,TREATMENT EFFECT IS STABLE ACROSS GEOGRAPHIES;,,
,NO SINGLE DMA DRIVES THE RESULT.,,
DESCRIPTION OF CHECKS,,,
1. Difference-in-Differences (DiD),,,
Compares the change in outcome between Treatment and Control groups from pre- to post-period.,,,
This provides a cross-check that the estimated lift is not merely due to different group trajectories,,,
over time.,,,
2. Pre-Period Balance Check,,,
A simple t-test comparing average pre-treatment outcomes between Treatment and Control groups.,,,
"A high p-value (e.g., >0.1) suggests the groups were balanced before treatment, reducing the risk",,,
of confounding.,,,
3. Leave-One-Out Sensitivity,,,
"Runs the treatment effect estimation multiple times, each time excluding one DMA from the",,,
"Treatment group. If the estimated lift remains stable across iterations, the result is not overly",,,
- Test Spend: $3.2MM,,,
- Incremental Revenue: $8.7MM,,,
- iROAS: 2.72,,,
- Recommendation: INCREASE social media budget by 20%,,,
ROBUSTNESS CHECKS:,,,
✓ DiD estimate: 3.38% (consistent),,,
✓ Leave-one-out: All estimates between 2.9% and 3.9%,,,
✓ Pre-period placebo: -0.21% (p = 0.84),,,
✓ Compliance: 96% of Treatment DMAs within range,,,
DATA QUALITY:,,,
- Sales data completeness: 99.7%,,,
- DMA assignment adherence: 100%,,,
- Spillover detected: <2% in 4 Control DMAs,,,
ROBUSTNESS CHECKS:,,,
✓ DiD estimate: 3.38% (consistent),,,
✓ Leave-one-out: All estimates between 2.9% and 3.9%,,,
✓ Pre-period placebo: -0.21% (p = 0.84),,,
✓ Compliance: 96% of Treatment DMAs within range,,,
DATA QUALITY:,,,
- Sales data completeness: 99.7%,,,
- DMA assignment adherence: 100%,,,
- Spillover detected: <2% in 4 Control DMAs,,,
"0 < LIFT < TARGETNOCO
LIFT ≤ 0ANYPA",,,
0 < LIFT < TARGET,NO,CO,
LIFT ≤ 0,ANY,PA,
• Is there a frequency threshold?,,,
Design follow-up experiments using the same,,,
rigorous framework.,,,
RECALIBRATING IMPACT RESPONSE CURVE IN MARKETING MI,,,
CHANNEL SPEND,,,
"▲ Illustration provided by MASS Analytics, MMM software pro",,,
Compare the Calibrated Curve with the one previously used in the model.,,,
RECALIBRATING IMPACT RESPONSE CURVE IN MARKETING MIX MODEL,,,
NEW CURVE,,,
OLD CURVE,,,
,RESPONSE CURVE BEFORE RCT (DR 60%),,
,RCT RESULTS,,
,RESPONSE CURVE AFTER RCT (DR 27%),,
CHANNEL SPEND,,,
"▲ Illustration provided by MASS Analytics, MMM software provider.",,,
Compare the Calibrated Curve with the one previously used in the model. Use,,,
smaller markets,,,
Solution:,,,
Solution: Include all DMAs; let randomization Fix delivery issues,,,
handle heterogeneity.,,,
PITFALL: CREATI,,,
PITFALL: PEEKING AT ASSIGNMENTS,,,
Example:,,,
Example:  “Let’s test the new,,,
"“Treatment got more large DMAs, let’s re-",,,
Why it’s wrong:  randomize”,,,
Confounds media e,,,
Why it’s wrong:,,,
Solution:  Not truly a randomized trail when treatment,,,
Keep all non-mediagroups are cherry-picked8,,,
creative testing or v,,,
Solution:  experiment,,,
Design test ahead of time with stratification or,,,
re-randomization with pre-specified criteria,,,
PITFALL: REACTI,,,
PITFALL: VAGUE SUCCESS CRITERIA Example:,,,
“Week 2 looks grea,,,
Example:,,,
Why it’s wrong:  “We’ll see if it works”,,,
Early results are no,,,
Why it’s wrong:,,,
Solution:  Invites post-hoc rationalization,,,
Stick to the plan; sa,,,
Solution:,,,
Define specific lift threshold and decision rule,,,
PITFALL: PEEKING AT ASSIGNMENTS,,,
Example:,,,
Example:  “Let’s test the new creative in Treatment DMAs”,,,
"“Treatment got more large DMAs, let’s re-",,,
Why it’s wrong:  randomize”,,,
Confounds media effect with creative effect,,,
Why it’s wrong:,,,
Solution:  Not truly a randomized trail when treatment,,,
Keep all non-media variables constant; Conduct groups are cherry-picked8,,,
creative testing or versioning as a separate,,,
Solution:  experiment,,,
Design test ahead of time with stratification or,,,
re-randomization with pre-specified criteria,,,
PITFALL: REACTING TO EARLY RESULTS,,,
PITFALL: VAGUE SUCCESS CRITERIA Example:,,,
"“Week 2 looks great, let’s double spending”",,,
Example:,,,
Why it’s wrong:  “We’ll see if it works”,,,
Early results are noisy; changes compromise test,,,
Why it’s wrong:,,,
Solution:  Invites post-hoc rationalization,,,
Stick to the plan; save learnings for next test,,,
Solution:,,,
Define specific lift threshold and decision rule,,,
Why it’s wrong:,,,
Imbalance can masquerade as treatment effect,,,
Solution:,,,
Use normalized metrics or covariate adjustment,,,
ADVANTAGES OF LARGE-SCALE GEOGRAPHIC RANDOMIZED CO,,,
• Scientifically sound: It is a true Randomized Control Trial,,,
"• Omnichannel: It works for cable TV, CTV, digital video, programmat",,,
"home, and more",,,
"• Advertiser-friendly: It is proven to work for advertisers, both large",,,
"• Extensible: Works for various KPIs including sales, foot traffic, TV tu",,,
• Independent: Requires nothing but plan compliance from media fir,,,
partners,,,
"• Low Tech: No special technology required (no clean rooms, user IDs,",,,
"• Privacy assured: No PII, only ZIP codes",,,
• Fraud proof: Performance cannot be gamed: we defy anyone to hack,,,
• Immediate results: Final report in minutes of last KPI data availabil,,,
"• Simple: Easily deployed in any media channel with DMA, ZIP code, o",,,
capabilities,,,
• Generalizable: Projectable to national campaigns because it uses all,,,
ADVANTAGES OF LARGE-SCALE GEOGRAPHIC RANDOMIZED CONTROLLED EXPERIMENT,,,
• Scientifically sound: It is a true Randomized Control Trial,,,
"• Omnichannel: It works for cable TV, CTV, digital video, programmatic, social, search, out-of-",,,
"home, and more",,,
"• Advertiser-friendly: It is proven to work for advertisers, both large and small",,,
"• Extensible: Works for various KPIs including sales, foot traffic, TV tune-in  and more",,,
"• Independent: Requires nothing but plan compliance from media firms, DSPs, agencies or other",,,
partners,,,
"• Low Tech: No special technology required (no clean rooms, user IDs, cookies, etc.)",,,
"• Privacy assured: No PII, only ZIP codes",,,
• Fraud proof: Performance cannot be gamed: we defy anyone to hack it,,,
• Immediate results: Final report in minutes of last KPI data availability,,,
"• Simple: Easily deployed in any media channel with DMA, ZIP code, or other geo targeting",,,
capabilities,,,
• Generalizable: Projectable to national campaigns because it uses all DMAs in the country in the,,,
more than just a methodology. It represents a improvements in e,,,
fundamental shift in how organizations should impact. A 5% impr,,,
approach marketing ROI. While competitors delivers far more v,,,
continue relying on correlation-based attribution in a $1M channel.,,,
models and quasi-experimental methods that systematically work,,,
"systematically overstate digital performance,  mix:",,,
companies that embrace geographic 1. Channel Vali,,,
experimentation gain a decisive edge. In markets channels to est,,,
where share is zero-sum and growth comes  contribution,,,
"at competitors’ expense, understanding what",,,
truly drives incremental sales becomes your  2. Risk Mitigati,,,
"secret weapon, demonstrably changing perception spending witho",,,
of marketing from a cost center  that appear ine,,,
into a growth accelerator. or with matche,,,
driving signific,,,
Opportunity WHILE COMPETITORS CONTINUE 3. underinvested,,,
RELYING ON CORRELATION-BASED and linear TV,,,
"ATTRIBUTION MODELS AND  and CPMs low,",,,
QUASI-EXPERIMENTAL METHODS  returns than ov,,,
THAT SYSTEMATICALLY OVERSTATE 4. Tactical Opti,,,
"DIGITAL PERFORMANCE,  level validation",,,
"COMPANIES THAT EMBRACE variants, audie",,,
GEOGRAPHIC EXPERIMENTATION  strategies,,,
GAIN A DECISIVE EDGE. 5. Continuous,,,
companies that embrace geographic 1.,Channel Validation: Test all major,,
experimentation gain a decisive edge. In markets,channels to establish true incremental,,
where share is zero-sum and growth comes,contribution,,
"at competitors’ expense, understanding what",,,
truly drives incremental sales becomes your  2.,Risk Mitigation: Never cut substantial,,
"secret weapon, demonstrably changing perception",spending without rigorous testing. Channels,,
of marketing from a cost center,that appear ineffective in attribution models,,
into a growth accelerator.,or with matched-market testing may be,,
,driving significant incremental sales,,
Opportunity Discovery: InvestigateWHILE COMPETITORS CONTINUE 3.,"underinvested channels like audio, outdoor,",,
RELYING ON CORRELATION-BASED,and linear TV where attention may be high,,
ATTRIBUTION MODELS AND,"and CPMs low, potentially offering better",,
QUASI-EXPERIMENTAL METHODS,returns than oversaturated digital channels,,
THAT SYSTEMATICALLY OVERSTATE 4.,Tactical Optimization: After channel-,,
"DIGITAL PERFORMANCE,","level validation, test media partners, creative",,
COMPANIES THAT EMBRACE,"variants, audience segments, and messaging",,
GEOGRAPHIC EXPERIMENTATION,strategies,,
GAIN A DECISIVE EDGE. 5.,Continuous Calibration: Track whether,,
,mix changes deliver expected sales impact,,
THE PATH FORWARD,"That said, executin",,
,well requires expert,,
Geographic RCTs offer simplicity in design but,sustained commit,,
profound impact in results. By randomly assigning,needs to build this,,
"regions, delivering media, and measuring",Central Control,,
"outcomes, you cut through the noise of modern","your partner, bri",,
marketing analytics. The careful attention to,methodology an,,
"design details, statistical power, operational",make geographi,,
"excellence, and analytical rigor pays dividends in",and optimal for,,
decision quality.,,,
,Remember: every f,,
This whitepaper demonstrates that while running,prevents wasted sp,,
"experiments requires rigor and attention to detail,",successful test that,,
it’s not as complicated or expensive as many,In the words often,,
"marketers fear. This isn’t rocket science,",“”I have not failed.,,
it’s marketing science.,that won’t work.“,,
The real cost isn’t in the resources required,Start today. Pick yo,,
or the temporary sales suspension in control,a test. Let real scien,,
markets. The real cost is making million-dollar,statistical mysticis,,
media decisions based on flawed measurement,dollar decision. Soo,,
while billions in revenue opportunity hang in the,of leading marketin,,
balance.,"like Netflix, Uber,",,
Every day spent optimizing against inaccurate,leaders and wonder,,
signals is a day your competitors might be using,without this level of,,
better evidence to steal your customers.,,,
"outcomes, you cut through the noise of modern","your partner, bringing battle-tested",,
marketing analytics. The careful attention to,methodology and platform capabilities to,,
"design details, statistical power, operational",make geographic experimentation painless,,
"excellence, and analytical rigor pays dividends in",and optimal for your business.,,
decision quality.,,,
,Remember: every failed experiment that,,
This whitepaper demonstrates that while running,prevents wasted spend is as valuable as a,,
"experiments requires rigor and attention to detail,",successful test that identifies a winning strategy.,,
it’s not as complicated or expensive as many,"In the words often attributed to Thomas Edison,",,
"marketers fear. This isn’t rocket science,","“”I have not failed. I’ve just found 10,000 ways",,
it’s marketing science.,that won’t work.“,,
The real cost isn’t in the resources required,Start today. Pick your largest channel. Design,,
or the temporary sales suspension in control,"a test. Let real science, not assumptions or",,
markets. The real cost is making million-dollar,"statistical mysticism, guide your next million-",,
media decisions based on flawed measurement,"dollar decision. Soon, you’ll be in the company",,
while billions in revenue opportunity hang in the,of leading marketing experimentation experts,,
balance.,"like Netflix, Uber, Airbnb and other market",,
Every day spent optimizing against inaccurate,leaders and wonder how you ever made decisions,,
signals is a day your competitors might be using,without this level of evidence.,,
better evidence to steal your customers.,,,
# Read DMA list,,,
dmas = pd.read_csv(“dma_list.csv”),,,
# Simple random assignment without replacement to avo,,,
"dmas[“arm”] = np.random.choice([“Treatment”, “Control",,,
"size=len(dmas),",,,
replace=False),,,
# Check and print group size balance,,,
group_counts = dmas[“arm”].value_counts(),,,
"print(“Group assignment counts:\n”, group_counts)",,,
# Save assignments,,,
"dmas.to_csv(“geoRCT_assignments.csv”, index=False)",,,
EXAMPLE 2: POWER ANALYSIS DATA STRUCTURE,,,
Referenced in: Historical Data Requirements for Simulation section,,,
# Example data structure for power analysis,,,
historical_data = pd.DataFrame({,,,
"‘dma_code’: [501, 501, 501, ...],",,,
"‘week_ending’: [‘2023-01-07’, ‘2023-01-14’, ...],",,,
"size=len(dmas),",,,
replace=False),,,
# Check and print group size balance,,,
group_counts = dmas[“arm”].value_counts(),,,
"print(“Group assignment counts:\n”, group_counts)",,,
# Save assignments,,,
"dmas.to_csv(“geoRCT_assignments.csv”, index=False)",,,
EXAMPLE 2: POWER ANALYSIS DATA STRUCTURE,,,
Referenced in: Historical Data Requirements for Simulation section,,,
# Example data structure for power analysis,,,
historical_data = pd.DataFrame({,,,
"‘dma_code’: [501, 501, 501, ...],",,,
"‘week_ending’: [‘2023-01-07’, ‘2023-01-14’, ...],",,,
var_total = sd_sales ** 2,,,
var_between = icc * var_total,,,
var_within = var_total - var_between,,,
# Generate random intercepts for each DMA,,,
"dma_effects = np.random.normal(0, np.sqrt(var_bet",,,
# Simulate weekly sales per DMA,,,
data = [],,,
for dma in range(n_dmas):,,,
for week in range(n_weeks):,,,
y = mean_sales + dma_effects[dma] + np.ra,,,
np.sqrt(var_within)),,,
"data.append({‘dma’: dma, ‘week’: week, ‘s",,,
return pd.DataFrame(data),,,
# Estimate ICC using mixed-effects model (robust to i,,,
def estimate_icc(df):,,,
"model = MixedLM.from_formula(‘sales ~ 1’, groups=",,,
result = model.fit(),,,
"var_between = result.cov_re.iloc[0, 0]# Random",,,
"dma_effects = np.random.normal(0, np.sqrt(var_between), size=n_dmas)",,,
# Simulate weekly sales per DMA,,,
data = [],,,
for dma in range(n_dmas):,,,
for week in range(n_weeks):,,,
"y = mean_sales + dma_effects[dma] + np.random.normal(0,",,,
np.sqrt(var_within)),,,
"data.append({‘dma’: dma, ‘week’: week, ‘sales’: y})",,,
return pd.DataFrame(data),,,
# Estimate ICC using mixed-effects model (robust to imbalance),,,
def estimate_icc(df):,,,
"model = MixedLM.from_formula(‘sales ~ 1’, groups=’dma’, data=df)",,,
result = model.fit(),,,
"var_between = result.cov_re.iloc[0, 0]# Random intercept variance",,,
# Apply lift to final week in treatment g,,,
df.loc[(df[‘group’] == ‘T’) & (df[‘week’],,,
‘sales’] *= (1 + lift),,,
# Difference-in-differences by DMA,,,
pre = df[df[‘week’] < weeks - 1].groupby(,,,
post = df[df[‘week’] == weeks - 1].groupb,,,
mean(),,,
did = (post - pre).reset_index().merge(df,,,
"drop_duplicates(), on=’dma’)",,,
"t, p = ttest_ind(did[did[‘group’] == ‘T’]",,,
did[did[‘group’] == ‘C’],,,
equal_var=False),,,
if p < 0.05:,,,
significant += 1,,,
power = significant / n_sim,,,
"results.append({‘weeks’: weeks, ‘power’: powe",,,
,# Difference-in-differences by DMA,,
,pre = df[df[‘week’] < weeks - 1].groupby(‘dma’)[‘sales’].mean(),,
,post = df[df[‘week’] == weeks - 1].groupby(‘dma’)[‘sales’].,,
mean(),,,
,"did = (post - pre).reset_index().merge(df[[‘dma’, ‘group’]].",,
"drop_duplicates(), on=’dma’)",,,
,"t, p = ttest_ind(did[did[‘group’] == ‘T’][‘sales’],",,
,"did[did[‘group’] == ‘C’][‘sales’],",,
,equal_var=False),,
,if p < 0.05:,,
,significant += 1,,
,power = significant / n_sim,,
,"results.append({‘weeks’: weeks, ‘power’: power})",,
return pd.DataFrame(results),,,
# Create database connection (replace with actual cre,,,
engine = create_engine(‘your_connection_string’),,,
# Define SQL query,,,
query = “””,,,
SELECT,,,
"customer_zip,",,,
"transaction_date,",,,
"sales_amount,-- for volume",,,
1 AS trans_count-- for counts,,,
FROM transactions,,,
WHERE transaction_date BETWEEN ‘2025-03-01’ AND ‘2025,,,
AND customer_zip IS NOT NULL,,,
“””,,,
# Execute and load into DataFrame,,,
"transactions = pd.read_sql(query, engine)",,,
query = “””,,,
SELECT,,,
"customer_zip,",,,
"transaction_date,",,,
"sales_amount,-- for volume",,,
1 AS trans_count-- for counts,,,
FROM transactions,,,
WHERE transaction_date BETWEEN ‘2025-03-01’ AND ‘2025-06-13’,,,
AND customer_zip IS NOT NULL,,,
“””,,,
# Execute and load into DataFrame,,,
"transactions = pd.read_sql(query, engine)",,,
transactions[‘transaction_date’] = pd.to_datetime(tra,,,
[‘transaction_date’]),,,
# Aggregate transactions to DMA-week level,,,
weekly_data = transactions.groupby(,,,
"[‘dma_code’, pd.Grouper(key=’transaction_date’, f",,,
).agg({,,,
"‘sales_amount’: ‘sum’,# Sum of sales for volum",,,
‘trans_count’: ‘sum’# Sum of transactions fo,,,
}).reset_index(),,,
EXAMPLE 6: DATA QUALITY CHECKS,,,
Referenced in: Data Quality Checks section,,,
# Verify completeness,,,
coverage = weekly_data.groupby('dma_code').size(),,,
"print(f""DMAs with full data: {(coverage == expected_w",,,
# Check for anomalies,,,
import matplotlib.pyplot as plt,,,
weekly_data.groupby('dma_code')['sales_amount'].plot(,,,
"[‘dma_code’, pd.Grouper(key=’transaction_date’, freq=’W’)]",,,
).agg({,,,
"‘sales_amount’: ‘sum’,# Sum of sales for volume",,,
‘trans_count’: ‘sum’# Sum of transactions for counts,,,
}).reset_index(),,,
EXAMPLE 6: DATA QUALITY CHECKS,,,
Referenced in: Data Quality Checks section,,,
# Verify completeness,,,
coverage = weekly_data.groupby('dma_code').size(),,,
"print(f""DMAs with full data: {(coverage == expected_weeks).sum()} / 210"")",,,
# Check for anomalies,,,
import matplotlib.pyplot as plt,,,
"weekly_data.groupby('dma_code')['sales_amount'].plot(figsize=(12, 8))",,,
plt.title('Sales by DMA Over Time'),,,
weekly_data['sales_index'] = weekly_data.apply(lambda,,,
"baseline_means[x['dma']], axis=1)",,,
# Test normalized values for approximate normality,,,
skew = stats.skew(weekly_data['sales_per_capita']),,,
if abs(skew) > 1:,,,
"print(f""Normalized sales skewness: {skew:.2f}"")",,,
"print(""Consider additional transformations if nee",,,
"assumptions"")",,,
EXAMPLE 7: PRIMARY ANALYSIS - T-TEST ON NORMALIZED VALU,,,
Referenced in: Primary Analysis section,,,
import pandas as pd,,,
import numpy as np,,,
from scipy import stats,,,
import statsmodels.api as sm,,,
from statsmodels.stats.anova import anova_lm,,,
from statsmodels.stats.sandwich_covariance import cov,,,
# Load DMA-level pre/post sales and assignment data,,,
if abs(skew) > 1:,,,
"print(f""Normalized sales skewness: {skew:.2f}"")",,,
"print(""Consider additional transformations if needed for model",,,
"assumptions"")",,,
EXAMPLE 7: PRIMARY ANALYSIS - T-TEST ON NORMALIZED VALUES,,,
Referenced in: Primary Analysis section,,,
import pandas as pd,,,
import numpy as np,,,
from scipy import stats,,,
import statsmodels.api as sm,,,
from statsmodels.stats.anova import anova_lm,,,
from statsmodels.stats.sandwich_covariance import cov_hc1,,,
# Load DMA-level pre/post sales and assignment data,,,
df = pd.read_csv(“geo_rct_results.csv”),,,
analysis_df = analysis_df.merge(,,,
"df[[‘dma_code’, ‘assignment’]].drop_duplicates(),",,,
on=’dma_code’,,,
),,,
# Step 4: Estimate expected sales using pre-period tr,,,
# Assumes linear growth and 5 weeks of test period,,,
analysis_df[‘expected_sales’] = analysis_df[‘pre_avg’,,,
df[‘pre_trend’] * 5),,,
# Step 5: Calculate normalized lift index,,,
analysis_df[‘lift_index’] = (analysis_df[‘test_avg’],,,
df[‘expected_sales’]) - 1,,,
# Step 6: Run OLS regression with treatment group as,,,
X = sm.add_constant(pd.get_dummies(analysis_df[‘assig,,,
"first=True))  # e.g., Control=0, Treatment=1",,,
y = analysis_df[‘lift_index’],,,
"model = sm.OLS(y, X).fit()",,,
print(model.summary()),,,
# Step 7: Report robust (HC1) standard errors,,,
robust_cov = cov_hc1(model),,,
robust_se = np.sqrt(np.diag(robust_cov)),,,
# Step 4: Estimate expected sales using pre-period trend,,,
# Assumes linear growth and 5 weeks of test period,,,
analysis_df[‘expected_sales’] = analysis_df[‘pre_avg’] * (1 + analysis_,,,
df[‘pre_trend’] * 5),,,
# Step 5: Calculate normalized lift index,,,
analysis_df[‘lift_index’] = (analysis_df[‘test_avg’] / analysis_,,,
df[‘expected_sales’]) - 1,,,
# Step 6: Run OLS regression with treatment group as predictor,,,
"X = sm.add_constant(pd.get_dummies(analysis_df[‘assignment’], drop_",,,
"first=True))  # e.g., Control=0, Treatment=1",,,
y = analysis_df[‘lift_index’],,,
"model = sm.OLS(y, X).fit()",,,
print(model.summary()),,,
# Step 7: Report robust (HC1) standard errors,,,
robust_cov = cov_hc1(model),,,
robust_se = np.sqrt(np.diag(robust_cov)),,,
).fit(,,,
"cov_type=’cluster’,",,,
cov_kwds={‘groups’: did_df[‘dma_code’]}  # cluste,,,
),,,
# Output the DiD interaction coefficient,,,
interaction_term = ‘assignment[T.Treatment]:post’,,,
if interaction_term in did_model.params:,,,
print(f”DiD estimate: {did_model.params[interacti,,,
else:,,,
print(“Interaction term not found in model output,,,
encoding.”),,,
EXAMPLE 9: LEAVE-ONE-OUT ANALYSIS,,,
Referenced in: Robustness Checks section,,,
import pandas as pd,,,
import statsmodels.api as sm,,,
import matplotlib.pyplot as plt,,,
# Run leave-one-out regression by dropping one DMA at,,,
loo_results = [],,,
# Output the DiD interaction coefficient,,,
interaction_term = ‘assignment[T.Treatment]:post’,,,
if interaction_term in did_model.params:,,,
print(f”DiD estimate: {did_model.params[interaction_term]:.4f}”),,,
else:,,,
print(“Interaction term not found in model output. Check data,,,
encoding.”),,,
EXAMPLE 9: LEAVE-ONE-OUT ANALYSIS,,,
Referenced in: Robustness Checks section,,,
import pandas as pd,,,
import statsmodels.api as sm,,,
import matplotlib.pyplot as plt,,,
# Run leave-one-out regression by dropping one DMA at a time,,,
loo_results = [],,,
# Plot LOO estimates to identify influential DMAs,,,
"plt.figure(figsize=(10, 6))",,,
"plt.scatter(loo_df[‘excluded_dma’], loo_df[‘estimate’",,,
"plt.axhline(y=0.0342, color=’red’, linestyle=’--’, la",,,
estimate’),,,
plt.xlabel(‘Excluded DMA’),,,
plt.ylabel(‘Treatment Effect Estimate’),,,
plt.title(‘Leave-One-Out Sensitivity Analysis’),,,
plt.legend(),,,
plt.tight_layout(),,,
plt.show(),,,
EXAMPLE 10: PRE-PERIOD BALANCE CHECK,,,
Referenced in: Robustness Checks section,,,
import pandas as pd,,,
import numpy as np,,,
import statsmodels.api as sm,,,
# Create fake pre/test periods using pre-period weeks,,,
placebo_df = df[df[‘period’] == ‘pre’].copy(),,,
estimate’),,,
plt.xlabel(‘Excluded DMA’),,,
plt.ylabel(‘Treatment Effect Estimate’),,,
plt.title(‘Leave-One-Out Sensitivity Analysis’),,,
plt.legend(),,,
plt.tight_layout(),,,
plt.show(),,,
EXAMPLE 10: PRE-PERIOD BALANCE CHECK,,,
Referenced in: Robustness Checks section,,,
import pandas as pd,,,
import numpy as np,,,
import statsmodels.api as sm,,,
# Create fake pre/test periods using pre-period weeks,,,
placebo_df = df[df[‘period’] == ‘pre’].copy(),,,
,on=’dma_code’,,
),,,
# Compute placebo lift (should be ~0 if pre-periods a,,,
placebo_pivot[‘fake_lift’] = (placebo_pivot[‘fake_tes,,,
pivot[‘fake_pre’]) - 1,,,
# Run placebo regression,,,
placebo_model = sm.OLS(,,,
,"placebo_pivot[‘fake_lift’],",,
,sm.add_constant(pd.get_dummies(placebo_pivot[‘ass,,
first=True)),,,
).fit(),,,
# Report placebo results,,,
print(f”Placebo effect: {placebo_model.params[1]:.4f},,,
print(f”Placebo p-value: {placebo_model.pvalues[1]:.4,,,
EXAMPLE 11: UPDATE MARKETING MIX MODEL WITH EXPERIMEN,,,
Referenced in: Feeding Back to Strategy section,,,
# Example: Updating MMM with experimental prior,,,
# Prior from experiment: 3.4% ± 1.5%,,,
pivot[‘fake_pre’]) - 1,,,
# Run placebo regression,,,
placebo_model = sm.OLS(,,,
"placebo_pivot[‘fake_lift’],",,,
"sm.add_constant(pd.get_dummies(placebo_pivot[‘assignment’], drop_",,,
first=True)),,,
).fit(),,,
# Report placebo results,,,
print(f”Placebo effect: {placebo_model.params[1]:.4f}”),,,
print(f”Placebo p-value: {placebo_model.pvalues[1]:.4f}”),,,
EXAMPLE 11: UPDATE MARKETING MIX MODEL WITH EXPERIMENTAL PRIOR,,,
Referenced in: Feeding Back to Strategy section,,,
# Example: Updating MMM with experimental prior,,,
# Prior from experiment: 3.4% ± 1.5%,,,
import pandas as pd,,,
import numpy as np,,,
from sklearn.preprocessing import StandardScaler,,,
from sklearn.cluster import KMeans,,,
"def stratified_randomization(dmas_df, strat_vars, n_s",,,
“””,,,
Stratified randomization for Geo RCT,,,
“””,,,
# Standardize stratification variables,,,
scaler = StandardScaler(),,,
X = scaler.fit_transform(dmas_df[strat_vars]),,,
# Create strata using k-means clustering,,,
"kmeans = KMeans(n_clusters=n_strata, random_state",,,
dmas_df[‘stratum’] = kmeans.fit_predict(X),,,
# Randomize within strata,,,
np.random.seed(seed),,,
dmas_df[‘assignment’] = ‘Control’,,,
for stratum in range(n_strata):,,,
from sklearn.cluster import KMeans,,,
"def stratified_randomization(dmas_df, strat_vars, n_strata=4, seed=42):",,,
“””,,,
Stratified randomization for Geo RCT,,,
“””,,,
# Standardize stratification variables,,,
scaler = StandardScaler(),,,
X = scaler.fit_transform(dmas_df[strat_vars]),,,
# Create strata using k-means clustering,,,
"kmeans = KMeans(n_clusters=n_strata, random_state=seed)",,,
dmas_df[‘stratum’] = kmeans.fit_predict(X),,,
# Randomize within strata,,,
np.random.seed(seed),,,
dmas_df[‘assignment’] = ‘Control’,,,
for stratum in range(n_strata):,,,
"assignments = stratified_randomization(dmas, strat_va",,,
EXAMPLE 13: POWER SIMULATION FRAMEWORK,,,
Referenced in: Simulation-Based Power Estimation section,,,
import pandas as pd,,,
import numpy as np,,,
"from joblib import Parallel, delayed",,,
from scipy.stats import ttest_ind,,,
"def run_power_simulation(historical_data, effect_size",,,
"test_weeks=[3, 4, 5, 6, 8],",,,
alpha=0.05):,,,
“””,,,
Run power simulation for geographic RCT,,,
“””,,,
results = [],,,
for effect in effect_sizes:,,,
for weeks in test_weeks:,,,
# Run simulations in parallel,,,
sim_results = Parallel(n_jobs=-1)(,,,
import pandas as pd,,,
import numpy as np,,,
"from joblib import Parallel, delayed",,,
from scipy.stats import ttest_ind,,,
"def run_power_simulation(historical_data, effect_sizes=[0.02, 0.03, 0.05],",,,
"test_weeks=[3, 4, 5, 6, 8], n_sims=1000,",,,
alpha=0.05):,,,
“””,,,
Run power simulation for geographic RCT,,,
“””,,,
results = [],,,
for effect in effect_sizes:,,,
for weeks in test_weeks:,,,
# Run simulations in parallel,,,
sim_results = Parallel(n_jobs=-1)(,,,
“””,,,
# Sample random test window (ensure space for 8-w,,,
"start_week = np.random.randint(8, len(historical_",,,
# Extract pre and test periods,,,
pre_data = historical_data.iloc[start_week - 8:st,,,
test_data = historical_data.iloc[start_week:start,,,
# Random assignment of DMAs to treatment and cont,,,
dmas = historical_data[‘dma_code’].unique(),,,
"treatment_dmas = np.random.choice(dmas, len(dmas)",,,
pre_data[‘group’] = pre_data[‘dma_code’].apply(la,,,
treatment_dmas else ‘C’),,,
test_data[‘group’] = test_data[‘dma_code’].apply(,,,
treatment_dmas else ‘C’),,,
# Aggregate weekly sales by DMA,,,
pre_avg = pre_data.groupby(‘dma_code’)[‘sales_vol,,,
index(name=’pre_avg’),,,
test_avg = test_data.groupby(‘dma_code’)[‘sales_v,,,
index(name=’test_avg’),,,
# Apply simulated lift to treatment group,,,
test_avg[‘group’] = test_avg[‘dma_code’].apply(la,,,
pre_data = historical_data.iloc[start_week - 8:start_week].copy(),,,
test_data = historical_data.iloc[start_week:start_week + weeks].copy(),,,
# Random assignment of DMAs to treatment and control,,,
dmas = historical_data[‘dma_code’].unique(),,,
"treatment_dmas = np.random.choice(dmas, len(dmas) // 2, replace=False)",,,
pre_data[‘group’] = pre_data[‘dma_code’].apply(lambda x: ‘T’ if x in,,,
treatment_dmas else ‘C’),,,
test_data[‘group’] = test_data[‘dma_code’].apply(lambda x: ‘T’ if x in,,,
treatment_dmas else ‘C’),,,
# Aggregate weekly sales by DMA,,,
pre_avg = pre_data.groupby(‘dma_code’)[‘sales_volume’].mean().reset_,,,
index(name=’pre_avg’),,,
test_avg = test_data.groupby(‘dma_code’)[‘sales_volume’].mean().reset_,,,
index(name=’test_avg’),,,
# Apply simulated lift to treatment group,,,
test_avg[‘group’] = test_avg[‘dma_code’].apply(lambda x: ‘T’ if x in,,,
treatment_dmas else ‘C’),,,
# print(power_results),,,
APPENDIX B: GLOSSARY,,,
ANCOVA: Analysis of Covariance; regression,MDE: Minimum D,,
model that includes treatment indicator and,true effect size that,,
continuous covariates,power to detect as s,,
Cluster-RCT: Cluster Randomized Controlled,MECE: Mutually E,,
Trial; experimental design where groups (clusters),Exhaustive; proper,,
rather than individuals are randomly assigned to,overlap and cover a,,
treatment conditions,MMM: Marketing,,
DMA: Designated Market Area; geographic,that decomposes sa,,
regions defined by Nielsen for television,various marketing,,
"viewership measurement, commonly used as",SMD: Standardize,,
experimental units in geo tests,in means divided b,,
ICC: Intraclass Correlation Coefficient; measures,used to assess bala,,
the similarity of observations within the same,SUTVA: Stable Un,,
cluster relative to observations between clusters,Assumption; assum,,
iROAS: Incremental Return on Ad Spend; the,unit doesn't affect o,,
"causal revenue impact per dollar spent, measured",spillover),,
through experimentation rather than correlation,,,
ITT: Intent-to-Treat; analysis based on initial,,,
ANCOVA: Analysis of Covariance; regression,MDE: Minimum Detectable Effect; the smallest,,
model that includes treatment indicator and,true effect size that an experiment has adequate,,
continuous covariates,power to detect as statistically significant,,
Cluster-RCT: Cluster Randomized Controlled,MECE: Mutually Exclusive and Collectively,,
Trial; experimental design where groups (clusters),Exhaustive; property where categories don't,,
rather than individuals are randomly assigned to,overlap and cover all possibilities,,
treatment conditions,MMM: Marketing Mix Model; statistical model,,
DMA: Designated Market Area; geographic,that decomposes sales into contributions from,,
regions defined by Nielsen for television,various marketing channels and external factors,,
"viewership measurement, commonly used as",SMD: Standardized Mean Difference; difference,,
experimental units in geo tests,"in means divided by pooled standard deviation,",,
ICC: Intraclass Correlation Coefficient; measures,used to assess balance between groups,,
the similarity of observations within the same,SUTVA: Stable Unit Treatment Value,,
cluster relative to observations between clusters,Assumption; assumption that treatment of one,,
iROAS: Incremental Return on Ad Spend; the,unit doesn't affect outcomes of other units (no,,
"causal revenue impact per dollar spent, measured",spillover),,
through experimentation rather than correlation,,,
ITT: Intent-to-Treat; analysis based on initial,,,
treatment assignment regardless of actual,,,
,Randomization code peer-reviewed,,DiD cross-chec
□,DMA assignments generated and locked,□,Leave-one-out
□,Version control established,□,Pre-period plac
,,□,Confidence inte
Media Readiness,,,
□,Platform targeting lists created,Documentation,
□,Creative assets approved and identical,□,Results summa
□,Trafficking instructions documented,□,Technical appe
□,Compliance monitoring dashboard live,□,Visualizations c
□,Pacing alerts configured,□,Recommendati
,,□,Lessons learne
Data Infrastructure,,,
□,KPI data pipeline tested,Action Items,
□,ZIP→DMA mapping current,□,Decision rule e
□,Latency documented and acceptable,□,Budget change
□,Analysis code templates ready,□,MMM coefficie
□,Results template prepared,□,Next test plann
,,□,Results socializ
Media Readiness,,,
□ Platform targeting lists created,Documentation,,
□ Creative assets approved and identical,□ Results summary drafted,,
□ Trafficking instructions documented,□ Technical appendix complete,,
□ Compliance monitoring dashboard live,□ Visualizations created,,
□ Pacing alerts configured,□ Recommendations clear,,
,□ Lessons learned captured,,
Data Infrastructure,,,
□ KPI data pipeline tested,Action Items,,
□ ZIP→DMA mapping current,□ Decision rule executed,,
□ Latency documented and acceptable,□ Budget changes implemented,,
□ Analysis code templates ready,□ MMM coefficients updated,,
□ Results template prepared,□ Next test planned,,
,□ Results socialized,,
"brands, media companies, agencies, and others",research and produ,,
to build a clearer understanding of advertising’s,"Google, Viacom, M",,
true effect on sales and other business outcomes.,Guideline. He is Vi,,
We specialize in helping organizations reframe,sciences industry b,,
how they assess advertising effectiveness for,moderator of the in,,
"competitive advantage, offering executive",online forum.,,
"advising, measurement retooling, and calibration",,,
of marketing mix models.,,,
The Central Control team includes ad industry,ACKNOWLEDGE,,
"veterans from Google, DoubleClick, Microsoft,",Various contributo,,
"Amazon, and Adobe, with deep expertise in",to the production o,,
"experimental design, media measurement, data",individuals: John C,,
"science, marketing analytics, econometrics, and",Science at Central,,
applied research.,of Data Insights LL,,
Our team has supported 500+ experiments for,Marketing at the U,,
"Fortune 500 advertisers, optimizing billions of",help in designing m,,
dollars in business impact.,techniques and tec,,
,Kumi Harischandr,,
,technical review of,,
,Chief Commercial,,
,"editing, and Ben M",,
,"Munday Design, fo",,
"competitive advantage, offering executive",online forum.,,
"advising, measurement retooling, and calibration",,,
of marketing mix models.,,,
The Central Control team includes ad industry,ACKNOWLEDGEMENTS:,,
"veterans from Google, DoubleClick, Microsoft,",Various contributors provided valuable input,,
"Amazon, and Adobe, with deep expertise in","to the production of this paper, namely these",,
"experimental design, media measurement, data","individuals: John Chandler, PhD, Head of Data",,
"science, marketing analytics, econometrics, and","Science at Central Control, Managing Partner",,
applied research.,of Data Insights LLC and Clinical Professor of,,
Our team has supported 500+ experiments for,"Marketing at the University of Montana, for",,
"Fortune 500 advertisers, optimizing billions of",help in designing many of these experimental,,
dollars in business impact.,techniques and technical review of the paper;,,
,"Kumi Harischandra, research scientist, for",,
,"technical review of the paper; Campbell Foster,",,
,"Chief Commercial Office of Central Control, for",,
,"editing, and Ben Munday, Creative Director of",,
,"Munday Design, for graphic design.",,
