EXAMPLE OF MULTI-ARMED GEOGRAPHIC RANDOMIZED,,,,,,,,,,
CONTROLLED EXPERIMENT USING ALL 210 DMAS,,,,,,,,,,
,GROUP A,,,,,,,,,
,,GROUP D,,,,,,,,
,GROUP B,,,,,,,,,
,,CONTROL GROUP,,,,,,,,
,GROUP C,,,,,,,,,
a reasonable planning target for most large,Specify the confidence level that will be used to,,,,,,,,,
consumer brands.,test for statistical significance. The standard is,,,,,,,,,
,"95%, corresponding to a significance threshold",,,,,,,,,
"It may feel odd to have to estimate this quantity,","(α) of 0.05, although 90% confidence is often",,,,,,,,,
since the point of the experiment is to determine,acceptable for marketing purposes.,,,,,,,,,
its value. Don’t think of this as a forecast.,,,,,,,,,,
"Instead, think of this as the minimum lift that",This determines the risk of a false positive (“Type,,,,,,,,,
would be worth detecting based on your business.,"I error”), i.e., concluding there was an effect when",,,,,,,,,
"A point of departure for this, if lacking any better","there wasn’t. A 95% confidence level implies that,",,,,,,,,,
"guidance, is what would be the minimum lift",under the null hypothesis (the opposite of the,,,,,,,,,
required to generate positive iROAS for,"hypothesis), there’s a 5% chance of incorrectly",,,,,,,,,
the campaign. The lift estimate is an input,declaring a statistically significant result.,,,,,,,,,
to power analysis that anchors the design in,,,,,,,,,,
real-world relevance.,TEMPORAL WINDOWS,,,,,,,,,
Power analysis will assess whether the proposed,,,,,,,,,,
design can detect this effect with sufficient,Pre-Period (Baseline) Length,,,,,,,,,
probability and may show that the setup is,Set a baseline window – eight weeks is usually,,,,,,,,,
sensitive enough to detect even smaller effects.,"sufficient, depending on sales cycles – to",,,,,,,,,
Avoid designing the experiment to detect only the,normalize each DMA’s trend before treatment.,,,,,,,,,
"absolute minimum effect you think might occur,",This period is used for trend stabilization and,,,,,,,,,
as this can overfit assumptions and raise the risk,covariate adjustment. Justify that it is long enough,,,,,,,,,
"360,000",,,,,,,,,,
"340,000",,,,,,,,,,
"320,000",,,,,,,,,,
"300,000",,,,,,,,,,
"280,000",,,,,,,,,,
"260,000",,,,,,,,,,
"240,000",,,,,,,,,,
"220,000",,,,,,,,,,
JAN,FEB,MAR,APR,MAY,1-Jun,JUL AUG,SEP,OCT,NOV,DEC
,,,,,MONTH,,,,,
"AVERAGE WEEKLY NATIONAL SALES, GROUPED BY MONTH",,,,,,"±1 STANDARD DEVIATION FROM THE MONTHLY AVERAGE, REFLECTING",,,,
,,,,,,NATURAL WEEK-TO-WEEK VARIATION ACROSS THE TWO-YEAR PERIOD,,,,
"▲ This line chart shows total weekly transactions across the U.S., grouped by month. While week-to-week variation is modest,",,,,,,,,,,
"a consistent seasonal pattern emerges: slower sales in early January, surges in mid-spring and summer, and a notable decline beginning",,,,,,,,,,
around Thanksgiving that continues through Christmas. This trend is based on real national sales data for a Fortune 500 company,,,,,,,,,,
and underscores the importance of pre-period normalization when designing geographic RCTs to control for seasonality that could,,,,,,,,,,
"50/50 for two arms, or 33/33/33 for three arms,",(or other KPI) data at the geographic level for both,,,,,,,,,
etc.) across all included DMAs. This ensures each,historical power analysis (ideally 2 years) and the,,,,,,,,,
geographic unit has the same chance of receiving,test period. Two primary sources are available:,,,,,,,,,
"treatment, preserving the statistical properties",,,,,,,,,,
needed for unbiased causal inference.,,,,,,,,,,
,First-Party CRM Data (Preferred):,,,,,,,,,
,• Extract daily or weekly transaction counts or,,,,,,,,,
Exclusion Criteria,sales volumes by ZIP code (daily preferred),,,,,,,,,
"Define up front which DMAs may be excluded,",• Requires: buyer ZIP code and transaction date,,,,,,,,,
and why. Valid reasons include things like low,(daily or weekly) for aggregated sales counts,,,,,,,,,
"store count, data sparsity, regulatory constraints,",,,,,,,,,,
or known structural anomalies. Aim to keep,• Provides maximum flexibility and granularity,,,,,,,,,
the full set of 210 DMAs unless exclusions are,• No additional licensing costs,,,,,,,,,
methodologically necessary. Excluding DMAs,,,,,,,,,,
"simply because they are large, account for a large",,,,,,,,,,
"portion of sales, or are competitive are not ideal",Third-Party Syndicated Data:,,,,,,,,,
"exclusion criteria, as their inclusion inherently","• Sources: NielsenIQ, Circana (formerly IRI),",,,,,,,,,
makes the results of the experiment more,or similar providers,,,,,,,,,
generalizable to real-world conditions. Forcing,,,,,,,,,,
"them into the test group is also a bad idea, moving",• Often pre-aggregated to DMA level,,,,,,,,,
out of the realm of being a randomized trial and,• Weekly granularity typical (daily may be cost-,,,,,,,,,
,,,Choose between transaction counts (categorical),,,,,,,
,,,or sales volume (continuous):,,,,,,,
DATE,SALES,ZIPCODE,"• Counts: More stable, less affected by outliers,",,,,,,,
,,,simpler statistical properties,,,,,,,
7/28/2024,18,99353,,,,,,,,
,,,• Volume: Captures full business impact but,,,,,,,
7/28/2024,16,99352,"may require transformation (log, square root) if",,,,,,,
,,,highly skewed,,,,,,,
7/28/2024,6,99350,,,,,,,,
,,,Document this choice as it affects power,,,,,,,
7/28/2024,24,99338,calculations and analysis methods.,,,,,,,
7/28/2024,2,99337,,,,,,,,
,,,"WHERE POSSIBLE, USE DAILY KPI",,,,,,,
7/28/2024,8,99336,"AGGREGATIONS, WHICH PROVIDE MORE",,,,,,,
7/28/2024,16,99324,STATISTICAL POWER THAN WEEKLY.,,,,,,,
,,,AVOID MONTHLY ROLL-UPS.,,,,,,,
7/28/2024,24,99323,,,,,,,,
7/28/2024,5,99320,,,,,,,,
7/28/2024,3,99301,,,,,,,,
"hypothesis true. Instead, it attempts to reject",that makes certain conclusions more or less,,,,,,,,,
the opposite – the null hypothesis. The null,plausible given our data and assumptions.,,,,,,,,,
hypothesis typically states there is no effect or that,,,,,,,,,,
the effect is less than the pre-specified minimum.,,,,,,,,,,
If the data are sufficiently inconsistent with the,RANDOMIZATION LOGIC,,,,,,,,,
"null hypothesis at your chosen confidence level,",,,,,,,,,,
you reject it in favor of your research hypothesis.,SIMPLE RANDOMIZATION AS THE DEFAULT,,,,,,,,,
,"At its core, randomization is straightforward: we",,,,,,,,,
,flip a fair coin (metaphorically) for each DMA to,,,,,,,,,
Example Based on Business Objectives:,determine whether it receives treatment or serves,,,,,,,,,
For the objective: “Measure iROAS of Meta as a,"as control. This simple approach, assigning each",,,,,,,,,
channel partner and renegotiate or reallocate if,of the 210 DMAs to treatment or control with,,,,,,,,,
"results are unfavorable,” the following hypothesis","equal probability, forms the foundation of our",,,,,,,,,
and null hypothesis could apply:,experimental design.,,,,,,,,,
,"(To see code for how it would work in practice, see",,,,,,,,,
Research Hypothesis (H1):,"Appendix A, Example 1: Simple Randomization",,,,,,,,,
Meta advertising will generate a statistically,for Geographic RCT.),,,,,,,,,
significant lift in sales revenue of at least 3% in,This approach might seem too simple – won’t,,,,,,,,,
treatment DMAs compared to control DMAs,random chance sometimes create imbalanced,,,,,,,,,
"during the 5-week test period, measured at the",groups? The key insight is that with 210 units and,,,,,,,,,
95% confidence level.,,,,,,,,,,
"2,136 sales per week (standard error = 335) compared to the Test group’s mean of 2,134 sales per week (standard error =368), with a two-sample t-test yielding t statistic = 0.03, p-value = 0.997, confirming no statistically significant differencebetween groups (<1% difference in means). The custom boxplot format displays the interquartile range (box), median(horizontal line within box), mean (thick horizontal line), and whiskers extending to mean ± 1 standard error, with nooutliers shown. These values from a Fortune 500 company’s actual sales data demonstrate how the Rolling Thunderframework naturally improves balance through random assignment without stratification or re-weighting, supportingrobust causal inference while maintaining simplicity, with DMA-level normalization in the analysis phase furtherensuring treatment effects aren’t confounded by residual baseline differences.",,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
100,,,,,,155,,,,
95,,,,,,172,,,,
90,,,,,,182196,,,,
85,,,,,,199,,,,
80,,,,,,END OF,,,,
,,,,,,PRE-PERIOD,,,,
1-Jan,15-Jan,1-Feb,15-Feb,1-Mar,15-Mar,,,,,
,,WEEK,,,,,,,,
▲ Pre-period Normalization  of Weekly Sales by DMA These line plots show raw and normalized weekly sales across 10,,,,,,,,,,
"example DMAs. The first panel (Raw Weekly Sales) reveals large differences in baseline market size and week-to-week variance,",,,,,,,,,,
obscuring comparability across regions. The second panel (Normalized Weekly Sales Index) adjusts each DMA to its own pre-period,,,,,,,,,,
"mean (set to 100), enabling meaningful comparisons of relative change.",,,,,,,,,,
Summary statistics confirm the effect of normalization: raw sales had a standard deviation of 263 and a coefficient of variation (CV),,,,,,,,,,
"of 0.165, whereas normalized sales had a standard deviation of 4.27 and a CV of 0.042. The sales data shown are actual transaction",,,,,,,,,,
records from a Fortune 500 compay. The balance shown between test and control cells – after normalization using each DMA’s,,,,,,,,,,
"pre-period trend – is genuine, not synthetic. This example demonstrates why stratification is typically unnecessary when using all",,,,,,,,,,
210 DMAs with this normalization method.,,,,,,,,,,
STRATIFIED RANDOMIZATION,measures that quantify how “far apart” two DMAs,,,,,,,,,
Groups DMAs by key characteristics before,,,,,,,,,,
,"are across multiple dimensions. Then, match",,,,,,,,,
randomizing within strata. (For sample,,,,,,,,,,
,each DMA with its most similar partner.,,,,,,,,,
"code, see Appendix A, Example 12: Stratified",,,,,,,,,,
,"Finally, use a randomization algorithm within",,,,,,,,,
Randomization.),,,,,,,,,,
,each pair to assign one to treatment and one,,,,,,,,,
,to control.,,,,,,,,,
This guarantees balance on stratification variables,,,,,,,,,,
but requires careful selection of stratifying,,,,,,,,,,
covariates. Best when you can identify 2-3,COVARIATE-CONSTRAINED RANDOMIZATION,,,,,,,,,
"clearly important variables (i.e., with strong",(RE-RANDOMIZATION),,,,,,,,,
correlation to KPI) and have sufficient DMAs,,,,,,,,,,
within each stratum.,,,,,,,,,,
,Generates multiple random assignments,,,,,,,,,
,"(e.g., 10,000 draws) and selects one meeting",,,,,,,,,
Example: Stratify by sales quartile to ensure,pre-specified balance criteria. Provides precise,,,,,,,,,
treatment and control arms have equal representation,control over balance without sacrificing,,,,,,,,,
"of high-, medium-, and low-volume markets.",randomization principles.6,,,,,,,,,
MATCHED-PAIR RANDOMIZATION,,,,,,,,,,
Pairs similar DMAs based on multiple,,,,,,,,,,
"characteristics (e.g., past sales trends, populations,",6 Rerandomization To Improve Covariate,,,,,,,,,
through regression or ANCOVA. Doesn’t change,a lift equal to or greater than the expected effect,,,,,,,,,
assignment but can recover power lost to,size. Link back to prior experiments or planning,,,,,,,,,
imbalance.,models to justify that the level of spend can drive,,,,,,,,,
These techniques become particularly valuable,a detectable effect.,,,,,,,,,
when:,,,,,,,,,,
"• Working with fewer than 210 DMAs (e.g.,",,,,,,,,,,
regional tests),MONITORING PLAN,,,,,,,,,
• Unable to run extensive historical simulations,Set up real-time dashboards or pacing reports to,,,,,,,,,
,ensure that delivery is on track. Allow for mid-,,,,,,,,,
• Stakeholders require demonstrable balance for,"flight correction without altering randomization,",,,,,,,,,
confidence,especially in high-stakes or high-budget tests.,,,,,,,,,
• Effect sizes are expected to be small (<2%),,,,,,,,,,
,TREATMENT-DELIVERY,,,,,,,,,
IMPORTANT DISTINCTION:,VERIFICATION,,,,,,,,,
RCT ENHANCEMENTS VS.,,,,,,,,,,
QUASI-EXPERIMENTAL METHODS,Establishing robust monitoring systems before launch ensures that the intended media exposure,,,,,,,,,
The approaches above are all enhancements,actually reaches treatment DMAs while staying,,,,,,,,,
within the RCT framework – they preserve,"out of control DMAs, a critical requirement for",,,,,,,,,
randomization while improving efficiency.,valid causal inference.,,,,,,,,,
of transaction date.” Define a cut-off date for,,,,,,,,,,
primary analysis to allow for complete data.,"must be specified during the design phase, not",,,,,,,,,
,added post hoc after seeing results.,,,,,,,,,
DATA AUDITS,,,,,,,,,,
Schedule at least one interim extract during the,,,,,,,,,,
,SECONDARY KPIS,,,,,,,,,
"test to verify record counts, completeness, and","If you must track additional metrics (e.g.,",,,,,,,,,
DMA-level mapping. Use this as a QA step to,"transactions, average order value, new-customer",,,,,,,,,
confirm integrity before final analysis.,"rate), pre-declare whether p-values will be",,,,,,,,,
,"corrected for multiple comparisons (e.g., Holm-",,,,,,,,,
,Bonferroni) or considered purely exploratory.,,,,,,,,,
BUSINESS SAFEGUARDS,Remember that each additional hypothesis test,,,,,,,,,
MINIMUM CONTROLS,increases the risk of false positives.,,,,,,,,,
"If relevant (e.g., in a cessation test), declare a",,,,,,,,,,
minimum media presence in control DMAs to,SUBGROUP ANALYSIS,,,,,,,,,
avoid business disruption or brand risk.,"Similarly, any planned subgroup analyses (e.g.,",,,,,,,,,
,"high- vs. low-income DMAs, urban vs. rural)",,,,,,,,,
EARLY-STOP CRITERIA,should be specified at the design phase. These will be tested using interaction terms in the primary,,,,,,,,,
"If conditional stopping is allowed, e.g., to prevent","model, not as separate experiments. Be cautious:",,,,,,,,,
,Step-by-Step Simulation Process:,,,,,,,,,
"For first-party data, this means:","(For sample code, see Appendix A, Example",,,,,,,,,
,13: Power Simulation Framework for detailed,,,,,,,,,
1. Two years of historical ZIP-level transactions,implementation.),,,,,,,,,
2. Same aggregation logic as planned for test,1. Randomly sample multiple possible test,,,,,,,,,
period,windows from the historical period. For a,,,,,,,,,
3. Same ZIP-to-DMA mapping to ensure,"planned 6-week test, we would sample dozens",,,,,,,,,
consistency,of different 6-week windows across the two,,,,,,,,,
,"years, preserving the calendar structure of",,,,,,,,,
,pre- and post-periods. This captures different,,,,,,,,,
For syndicated data:,seasonal contexts – a test in Q4 may behave,,,,,,,,,
1. Negotiate access to historical data (ideally,differently than Q2.,,,,,,,,,
2 years),2. Vary the treatment duration,,,,,,,,,
2. Verify no methodology changes during,"systematically. Run parallel simulations at 3,",,,,,,,,,
this period,"4, 5, 6, 8, 10, and 12 weeks. While 4-6 weeks",,,,,,,,,
is often the sweet spot for balancing cost and3. Account for any markets with incomplete,,,,,,,,,,
"statistical power, only simulation revealshistory",,,,,,,,,,
,the actual trade-offs for your specific sales,,,,,,,,,
The power simulation will use this exact data,patterns.,,,,,,,,,
"structure, preserving real-world variance patterns,",,,,,,,,,,
,3. Apply the DMA normalization process,,,,,,,,,
,• Compute the week-over-week growth rate,"8. Repeat this process thousands of times,",,,,,,,,
,during this baseline,"with different test windows, treatment",,,,,,,,
,• Project expected sales during the test period,"assignments, and durations to build a robust",,,,,,,,
,based on this trend,empirical distribution of power outcomes.,,,,,,,,
,• Express actual test-period sales as an index,,,,,,,,,
,relative to this projection. This normalization,,,,,,,,,
,is crucial: it transforms raw sales (which,KEY SIMULATION OUTPUTS,,,,,,,,
,vary greatly by DMA size) into comparable,,,,,,,,,
,"lift indices, dramatically reducing variance.",The Monte Carlo simulation yields several,,,,,,,,
,,critical insights:,,,,,,,,
4,Randomly assign DMAs to treatment,,,,,,,,,
,and control groups using the same,• Power curves by duration: Shows,,,,,,,,
,randomization logic planned for the,"probability of detecting effects at 3%, 5%, 7% lift",,,,,,,,
,real experiment (typically equal odds of,for each test length,,,,,,,,
,assignment to all experiment arms).,• Minimum detectable effect (MDE): The,,,,,,,,
5,Simulate a treatment effect by inflating,smallest lift where power exceeds 80%,,,,,,,,
,normalized sales values in the treatment,• Optimal test configuration: The duration that,,,,,,,,
,group during the test period by the,minimizes MDE while keeping the test practical,,,,,,,,
,"hypothesized effect size (e.g., 3%, 5%). This",,,,,,,,,
,creates the “signal” we’re trying to detect,,,,,,,,,
,against the “noise” of natural variation.,"For example, simulations might reveal:",,,,,,,,
weeks (or switching to daily data) are needed to,Key decision points:,,,,,,,,,
compensate for the lack of independence. Because,1. If power is too low at business-relevant effect sizes:,,,,,,,,,
we treat the number of DMAs as fixed (typically all,• Extend test duration (most common solution),,,,,,,,,
"210), we must adjust for ICC through test duration",• Increase media weight to drive larger effects,,,,,,,,,
and data granularity:,"• Broaden KPI scope (e.g., full brand line vs.",,,,,,,,,
,single product),,,,,,,,,
Practical Adjustments for High ICC:,• Switch to daily data if available,,,,,,,,,
1. Extend the test window to observe more,2. If power is very high (>95%):,,,,,,,,,
"time points. If ICC = 0.15, you might need 6",• Consider shortening the test to save budget,,,,,,,,,
weeks instead of 4 to achieve target power.,• Test more subtle tactics or lower spend levels,,,,,,,,,
,• Run multiple sequential tests instead,,,,,,,,,
"2. Switch to daily data, if day-to-day variation",of one long test,,,,,,,,,
adds usable signal. Daily data often shows,,,,,,,,,,
"lower ICC than weekly aggregates, effectively",3. Document the final configuration:,,,,,,,,,
increasing sample size.,• “5-week test achieves 83% power to detect,,,,,,,,,
,3% lift”,,,,,,,,,
3. Run parallel simulations with different,• “Daily data improves MDE from 3.5%,,,,,,,,,
granularities and durations to identify the,to 2.8% vs. weekly”,,,,,,,,,
most power-efficient combination. Sometimes,• “Extending beyond 6 weeks provides,,,,,,,,,
4 weeks of daily data outperforms 6 weeks of,minimal power gain”,,,,,,,,,
weekly data.,,,,,,,,,,
Randomization,"Simple random assignment, equal probability",,,,,,,,,
Random Seed,42 (stored in repo: geoRCT_Q2_2025_assignments.csv),,,,,,,,,
Pre-Period,"8 weeks (Mar 1 - Apr 25, 2025)",,,,,,,,,
Treatment Period,"5 weeks (Apr 26 - May 30, 2025)",,,,,,,,,
Post-Period,"2 weeks (May 31 - Jun 13, 2025) [washout buffer]",,,,,,,,,
Treatment Dosage,"10M impressions per week @ $10 CPM (≈$500,000 total)",,,,,,,,,
Media Channels,"Facebook, Instagram",,,,,,,,,
Confidence Level,95% (α = 0.05),,,,,,,,,
Power Target,≥80%,,,,,,,,,
Simulation Results,84% power to detect 3% lift (via 2 years historical data),,,,,,,,,
,MDE = 2.7% (minimum detectable effect at 80% power),,,,,,,,,
Normalization Method,8-week pre-period trend per DMA,,,,,,,,,
Analysis Method,T-test  on normalized sales indices,,,,,,,,,
Compliance Threshold,Treatment DMAs must receive 90-110% of planned spend,,,,,,,,,
Exclusions,None (all 210 DMAs included),,,,,,,,,
Secondary Analyses,None pre-specified,,,,,,,,,
CONTROL,"TALLAHASSEE-THOMASVILLE, YAKIMA-PASCO-RCHLND-KNNWCK, BAKERSFIELD, BALTIMORE, EUGENE, MINNEAPOLIS-SAINT PAUL, PEORIA-BLOOMINGTON, LITTLE ROCK-PINE BLUFF, BEND, OR, TRI-CITIES, TN-VA, WASHINGTON, DC-HAGRSTWN, SAN FRANCISCO-OAKLAND-SAN",,,,,,,,,
,"JOSE, MADISON, PARKERSBURG, HARTFORD-NEW HAVEN, BUTTE-BOZEMAN, SAINT LOUIS, SOUTH BEND-ELKHART, AMARILLO, ROANOKE-",,,,,,,,,
,"LYNCHBURG, FRESNO-VISALIA, SYRACUSE, BILLINGS, CHICO-REDDING, ALBUQUERQUE-SANTA FE, JUNEAU, TYLER-LONGVIEW (LFKN&NCGD)",,,,,,,,,
GROUP A,"QUINCY-HANNIBAL-KEOKUK, TULSA, JACKSON, TN, PORTLAND-AUBURN, UTICA, BATON ROUGE, COLUMBIA-JEFFERSON CITY, LIMA, COLORADO SPRINGS-PUEBLO, HOUSTON, ALPENA, TOLEDO, SAN DIEGO, MISSOULA, OTTUMWA-KIRKSVILLE, ALBANY-SCHENECTADY-TROY, SPRINGFIELD-",,,,,,,,,
,"HOLYOKE, HARRISONBURG, GREENSBORO-HIGH POINT-WINSTON SALEM, COLUMBUS-TUPELO-WEST POINT, COLUMBUS, GA, WICHITA FALLS-",,,,,,,,,
,"LAWTON, WAUSAU-RHINELANDER, WILKES BARRE-SCRANTON, HARRISBURG-LANCASTER-LEBANON-YORK, HUNTSVILLE-DECATUR-FLORENCE",,,,,,,,,
GROUP B,"MONROE-EL DORADO, ANCHORAGE, AUGUSTA, SEATTLE-TACOMA, MYRTLE BEACH-FLORENCE, MIAMI-FORT LAUDERDALE, DALLAS-FORT WORTH, LUBBOCK, LA CROSSE-EAU CLAIRE, CHICAGO, LOUISVILLE, ROCKFORD, ODESSA-MIDLAND, NEW YORK, ROCHESTER-MASON CITY-",,,,,,,,,
,"AUSTIN, CLEVELAND-AKRON, RENO, ELMIRA, HELENA, MILWAUKEE, MINOT-BISMARCK-DICKINSON, TWIN FALLS, BOSTON, LAFAYETTE, IN,",,,,,,,,,
,"BLUEFIELD-BECKLEY-OAK HILL, LINCOLN-HASTINGS-KEARNEY PLUS",,,,,,,,,
GROUP C,"MOBILE-PENSACOLA, JOHNSTOWN-ALTOONA, SPRINGFIELD, MO, TRAVERSE CITY-CADILLAC, NEW ORLEANS, LOS ANGELES, DULUTH-SUPERIOR, WEST PALM BEACH-FT PIERCE, NORTH PLATTE, JOPLIN-PITTSBURG, CLARKSBURG-WESTON, FORT WAYNE, WACO-TEMPLE-BRYAN, PRESQUE",,,,,,,,,
,"ISLE, GREENVILLE-SPARTANBURG-ASHEVILLE, DENVER, CHARLOTTE, SAN ANTONIO, PORTLAND, OR, FARGO-VALLEY CITY, AUSTIN,",,,,,,,,,
,"MONTEREY-SALINAS, OKLAHOMA CITY, CASPER-RIVERTON, TERRE HAUTE, IDAHO FALLS-POCATELLO",,,,,,,,,
GROUP D,"BILOXI-GULFPORT, LAKE CHARLES, YOUNGSTOWN, MANKATO, LAREDO, DES MOINES-AMES, WATERTOWN, OMAHA, MARQUETTE, JONESBORO, LANSING, JACKSONVILLE, PANAMA CITY, ATLANTA, BANGOR, CORPUS CHRISTI, DAYTON, PHILADELPHIA, CHEYENNE-SCOTTSBLUFF,",,,,,,,,,
,"PADUCAH-CAPE GIRARDEAU-HARRISBURG, GLENDIVE, ERIE, DETROIT, MEDFORD-KLAMATH FALLS, SHREVEPORT, KANSAS CITY",,,,,,,,,
GROUP E,"EUREKA, PHOENIX, GREAT FALLS, RAPID CITY, SHERMAN-ADA, SALT LAKE CITY, CHARLESTON-HUNTINGTON, BINGHAMTON, FLINT-SAGINAW-BAY CITY, COLUMBIA, SC, ABILENE-SWEETWATER, CHAMPAIGN-SPRINGFIELD-DECATUR, KNOXVILLE, SPOKANE, EL PASO, CHATTANOOGA,",,,,,,,,,
,"SIOUX FALLS (MITCHELL), GREEN BAY-APPLETON, SIOUX CITY, FAIRBANKS, CHARLESTON, SC, WHEELING-STEUBENVILLE, FORT SMITH-",,,,,,,,,
,"FAY-SPRNGDL, BIRMINGHAM, BOISE, ORLANDO-DAYTONA BEACH-MELBOURNE",,,,,,,,,
GROUP F,"HATTIESBURG-LAUREL, BURLINGTON-PLATTSBURGH, LAS VEGAS, ALEXANDRIA, LA, DOTHAN, MEMPHIS, RICHMOND-PETERSBURG, MONTGOMERY, BEAUMONT-PORT ARTHUR, WILMINGTON, BUFFALO, MERIDIAN, GRAND JUNCTION-MONTROSE, GREENWOOD-GREENVILLE, LEXINGTON, GRAND",,,,,,,,,
,"RAPIDS-KALAMAZOO-BATTLE CREEK, SAVANNAH, PITTSBURGH, CEDAR RAPIDS-WATERLOO-DUBUQUE, MACON, NASHVILLE, SANTA BARBARA-SAN",,,,,,,,,
,"MAR-SAN LUIS OBISPO, SACRAMENTO-STOCKTON-MODESTO, BOWLING GREEN, JACKSON, MS, INDIANAPOLIS",,,,,,,,,
,"DAVENPORT-ROCK ISLAND-MOLINE, LAFAYETTE, LA, FORT MYERS-NAPLES, CINCINNATI, TUCSON-SIERRA VISTA, HARLINGEN-WESLACO-",,,,,,,,,
proportional to pre-test KPI rate,,,,,,,,,,
Data Pipeline Verification:,,,,,,,,,,
□ Historical sales data passes integrity checks,,,,,,,,,,
□ ZIP→DMA mapping uses current DMA,,,,,,,,,,
definitions consistent with the medium’s,,,,,,,,,,
targeting,,,,,,,,,,
□ Sales reporting latency documented,,,,,,,,,,
and acceptable,,,,,,,,,,
□ Compliance monitoring dashboard live,,,,,,,,,,
□ Backup data extraction plan in place,,,,,,,,,,
Friday : Send compliance report to stakeholders,,,,,,,,,,
CRITICAL RULES:,,,,,,,,,,
1. Never change DMA assignments - This breaks randomization,,,,,,,,,,
2. Document all adjustments - Include timestamp and rationale,,,,,,,,,,
3. Don’t peek at results - Avoid the temptation to check lift mid-flight unless conditional stopping,,,,,,,,,,
was pre-established,,,,,,,,,,
"4. Maintain spend ratios - If cutting budget is necessary, due to business conditions, cut",,,,,,,,,,
proportionally across all DMAs,,,,,,,,,,
dates,,,,,,,,,,
Problem: Platform reporting discrepancies,,,,,,,,,,
"For example, Facebook Ads Manager shows 1,200",,,,,,,,,,
conversions while Google Analytics shows 850 for,,,,,,,,,,
the same campaign due to different attribution,,,,,,,,,,
windows (view-through vs. click-only) and,,,,,,,,,,
tracking methods.,,,,,,,,,,
• Solution: Designate one platform as,,,,,,,,,,
“source of truth”,,,,,,,,,,
• Reconcile differences post-campaign,,,,,,,,,,
Problem: Competitive activity spike,,,,,,,,,,
• Solution: Continue as planned; document,,,,,,,,,,
for post-analysis,,,,,,,,,,
• Do not add markets or extend test reactively,,,,,,,,,,
in test),The coefficient on assignment Treatment,,,,,,,,,
• Metrics: [Total sales volume / unit sales /,represents the estimated lift. For example:,,,,,,,,,
transactions],,,,,,,,,,
• Time period: [Full date range including pre and,Estimate: 0.0342,,,,,,,,,
post periods],,,,,,,,,,
,Std. Error: 0.0156,,,,,,,,,
"• Granularity: Daily (preferred, more statistical",,,,,,,,,,
power) or weekly,t value: 2.19,,,,,,,,,
Allow 5-7 business days after period close for,p-value: 0.029,,,,,,,,,
"transaction settlement and data processing, then","95% CI: [0.0036, 0.0648]",,,,,,,,,
freeze the dataset for analysis.,,,,,,,,,,
,"This indicates a 3.42% lift (p = 0.029), statistically",,,,,,,,,
DATA QUALITY CHECKS,significant at the 95% confidence level.,,,,,,,,,
"If using sales volume (continuous), check",,,,,,,,,,
distribution of normalized values.,,,,,,,,,,
"For count data, standard t-test typically suffices",,,,,,,,,,
"unless counts are very low (< 5 per DMA-week), in",,,,,,,,,,
which case consider Poisson regression.,,,,,,,,,,
(T-TEST),GROUPS WERE WELL BALANCED AT BASELINE.,,,,,,,,,
LEAVE-ONE-OUT (MEAN EFFECT) 3.61,TREATMENT EFFECT IS STABLE ACROSS GEOGRAPHIES;,,,,,,,,,
,NO SINGLE DMA DRIVES THE RESULT.,,,,,,,,,
DESCRIPTION OF CHECKS,,,,,,,,,,
1. Difference-in-Differences (DiD),,,,,,,,,,
Compares the change in outcome between Treatment and Control groups from pre- to post-period.,,,,,,,,,,
This provides a cross-check that the estimated lift is not merely due to different group trajectories,,,,,,,,,,
over time.,,,,,,,,,,
2. Pre-Period Balance Check,,,,,,,,,,
A simple t-test comparing average pre-treatment outcomes between Treatment and Control groups.,,,,,,,,,,
"A high p-value (e.g., >0.1) suggests the groups were balanced before treatment, reducing the risk",,,,,,,,,,
of confounding.,,,,,,,,,,
3. Leave-One-Out Sensitivity,,,,,,,,,,
"Runs the treatment effect estimation multiple times, each time excluding one DMA from the",,,,,,,,,,
"Treatment group. If the estimated lift remains stable across iterations, the result is not overly",,,,,,,,,,
ROBUSTNESS CHECKS:,,,,,,,,,,
✓ DiD estimate: 3.38% (consistent),,,,,,,,,,
✓ Leave-one-out: All estimates between 2.9% and 3.9%,,,,,,,,,,
✓ Pre-period placebo: -0.21% (p = 0.84),,,,,,,,,,
✓ Compliance: 96% of Treatment DMAs within range,,,,,,,,,,
DATA QUALITY:,,,,,,,,,,
- Sales data completeness: 99.7%,,,,,,,,,,
- DMA assignment adherence: 100%,,,,,,,,,,
- Spillover detected: <2% in 4 Control DMAs,,,,,,,,,,
RECALIBRATING IMPACT RESPONSE CURVE IN MARKETING MIX MODEL,,,,,,,,,,
NEW CURVE,,,,,,,,,,
OLD CURVE,,,,,,,,,,
,RESPONSE CURVE BEFORE RCT (DR 60%),,,,,,,,,
,RCT RESULTS,,,,,,,,,
,RESPONSE CURVE AFTER RCT (DR 27%),,,,,,,,,
CHANNEL SPEND,,,,,,,,,,
"▲ Illustration provided by MASS Analytics, MMM software provider.",,,,,,,,,,
Compare the Calibrated Curve with the one previously used in the model. Use,,,,,,,,,,
PITFALL: PEEKING AT ASSIGNMENTS,,,,,,,,,,
Example:,,,,,,,,,,
Example:  “Let’s test the new creative in Treatment DMAs”,,,,,,,,,,
"“Treatment got more large DMAs, let’s re-",,,,,,,,,,
Why it’s wrong:  randomize”,,,,,,,,,,
Confounds media effect with creative effect,,,,,,,,,,
Why it’s wrong:,,,,,,,,,,
Solution:  Not truly a randomized trail when treatment,,,,,,,,,,
Keep all non-media variables constant; Conduct groups are cherry-picked8,,,,,,,,,,
creative testing or versioning as a separate,,,,,,,,,,
Solution:  experiment,,,,,,,,,,
Design test ahead of time with stratification or,,,,,,,,,,
re-randomization with pre-specified criteria,,,,,,,,,,
PITFALL: REACTING TO EARLY RESULTS,,,,,,,,,,
PITFALL: VAGUE SUCCESS CRITERIA Example:,,,,,,,,,,
"“Week 2 looks great, let’s double spending”",,,,,,,,,,
Example:,,,,,,,,,,
Why it’s wrong:  “We’ll see if it works”,,,,,,,,,,
Early results are noisy; changes compromise test,,,,,,,,,,
Why it’s wrong:,,,,,,,,,,
Solution:  Invites post-hoc rationalization,,,,,,,,,,
Stick to the plan; save learnings for next test,,,,,,,,,,
Solution:,,,,,,,,,,
Define specific lift threshold and decision rule,,,,,,,,,,
ADVANTAGES OF LARGE-SCALE GEOGRAPHIC RANDOMIZED CONTROLLED EXPERIMENT,,,,,,,,,,
• Scientifically sound: It is a true Randomized Control Trial,,,,,,,,,,
"• Omnichannel: It works for cable TV, CTV, digital video, programmatic, social, search, out-of-",,,,,,,,,,
"home, and more",,,,,,,,,,
"• Advertiser-friendly: It is proven to work for advertisers, both large and small",,,,,,,,,,
"• Extensible: Works for various KPIs including sales, foot traffic, TV tune-in  and more",,,,,,,,,,
"• Independent: Requires nothing but plan compliance from media firms, DSPs, agencies or other",,,,,,,,,,
partners,,,,,,,,,,
"• Low Tech: No special technology required (no clean rooms, user IDs, cookies, etc.)",,,,,,,,,,
"• Privacy assured: No PII, only ZIP codes",,,,,,,,,,
• Fraud proof: Performance cannot be gamed: we defy anyone to hack it,,,,,,,,,,
• Immediate results: Final report in minutes of last KPI data availability,,,,,,,,,,
"• Simple: Easily deployed in any media channel with DMA, ZIP code, or other geo targeting",,,,,,,,,,
capabilities,,,,,,,,,,
• Generalizable: Projectable to national campaigns because it uses all DMAs in the country in the,,,,,,,,,,
companies that embrace geographic 1.,Channel Validation: Test all major,,,,,,,,,
experimentation gain a decisive edge. In markets,channels to establish true incremental,,,,,,,,,
where share is zero-sum and growth comes,contribution,,,,,,,,,
"at competitors’ expense, understanding what",,,,,,,,,,
truly drives incremental sales becomes your  2.,Risk Mitigation: Never cut substantial,,,,,,,,,
"secret weapon, demonstrably changing perception",spending without rigorous testing. Channels,,,,,,,,,
of marketing from a cost center,that appear ineffective in attribution models,,,,,,,,,
into a growth accelerator.,or with matched-market testing may be,,,,,,,,,
,driving significant incremental sales,,,,,,,,,
Opportunity Discovery: InvestigateWHILE COMPETITORS CONTINUE 3.,"underinvested channels like audio, outdoor,",,,,,,,,,
RELYING ON CORRELATION-BASED,and linear TV where attention may be high,,,,,,,,,
ATTRIBUTION MODELS AND,"and CPMs low, potentially offering better",,,,,,,,,
QUASI-EXPERIMENTAL METHODS,returns than oversaturated digital channels,,,,,,,,,
THAT SYSTEMATICALLY OVERSTATE 4.,Tactical Optimization: After channel-,,,,,,,,,
"DIGITAL PERFORMANCE,","level validation, test media partners, creative",,,,,,,,,
COMPANIES THAT EMBRACE,"variants, audience segments, and messaging",,,,,,,,,
GEOGRAPHIC EXPERIMENTATION,strategies,,,,,,,,,
GAIN A DECISIVE EDGE. 5.,Continuous Calibration: Track whether,,,,,,,,,
,mix changes deliver expected sales impact,,,,,,,,,
"outcomes, you cut through the noise of modern","your partner, bringing battle-tested",,,,,,,,,
marketing analytics. The careful attention to,methodology and platform capabilities to,,,,,,,,,
"design details, statistical power, operational",make geographic experimentation painless,,,,,,,,,
"excellence, and analytical rigor pays dividends in",and optimal for your business.,,,,,,,,,
decision quality.,,,,,,,,,,
,Remember: every failed experiment that,,,,,,,,,
This whitepaper demonstrates that while running,prevents wasted spend is as valuable as a,,,,,,,,,
"experiments requires rigor and attention to detail,",successful test that identifies a winning strategy.,,,,,,,,,
it’s not as complicated or expensive as many,"In the words often attributed to Thomas Edison,",,,,,,,,,
"marketers fear. This isn’t rocket science,","“”I have not failed. I’ve just found 10,000 ways",,,,,,,,,
it’s marketing science.,that won’t work.“,,,,,,,,,
The real cost isn’t in the resources required,Start today. Pick your largest channel. Design,,,,,,,,,
or the temporary sales suspension in control,"a test. Let real science, not assumptions or",,,,,,,,,
markets. The real cost is making million-dollar,"statistical mysticism, guide your next million-",,,,,,,,,
media decisions based on flawed measurement,"dollar decision. Soon, you’ll be in the company",,,,,,,,,
while billions in revenue opportunity hang in the,of leading marketing experimentation experts,,,,,,,,,
balance.,"like Netflix, Uber, Airbnb and other market",,,,,,,,,
Every day spent optimizing against inaccurate,leaders and wonder how you ever made decisions,,,,,,,,,
signals is a day your competitors might be using,without this level of evidence.,,,,,,,,,
better evidence to steal your customers.,,,,,,,,,,
"size=len(dmas),",,,,,,,,,,
replace=False),,,,,,,,,,
# Check and print group size balance,,,,,,,,,,
group_counts = dmas[“arm”].value_counts(),,,,,,,,,,
"print(“Group assignment counts:\n”, group_counts)",,,,,,,,,,
# Save assignments,,,,,,,,,,
"dmas.to_csv(“geoRCT_assignments.csv”, index=False)",,,,,,,,,,
EXAMPLE 2: POWER ANALYSIS DATA STRUCTURE,,,,,,,,,,
Referenced in: Historical Data Requirements for Simulation section,,,,,,,,,,
# Example data structure for power analysis,,,,,,,,,,
historical_data = pd.DataFrame({,,,,,,,,,,
"‘dma_code’: [501, 501, 501, ...],",,,,,,,,,,
"‘week_ending’: [‘2023-01-07’, ‘2023-01-14’, ...],",,,,,,,,,,
"dma_effects = np.random.normal(0, np.sqrt(var_between), size=n_dmas)",,,,,,,,,,
# Simulate weekly sales per DMA,,,,,,,,,,
data = [],,,,,,,,,,
for dma in range(n_dmas):,,,,,,,,,,
for week in range(n_weeks):,,,,,,,,,,
"y = mean_sales + dma_effects[dma] + np.random.normal(0,",,,,,,,,,,
np.sqrt(var_within)),,,,,,,,,,
"data.append({‘dma’: dma, ‘week’: week, ‘sales’: y})",,,,,,,,,,
return pd.DataFrame(data),,,,,,,,,,
# Estimate ICC using mixed-effects model (robust to imbalance),,,,,,,,,,
def estimate_icc(df):,,,,,,,,,,
"model = MixedLM.from_formula(‘sales ~ 1’, groups=’dma’, data=df)",,,,,,,,,,
result = model.fit(),,,,,,,,,,
"var_between = result.cov_re.iloc[0, 0]# Random intercept variance",,,,,,,,,,
,# Difference-in-differences by DMA,,,,,,,,,
,pre = df[df[‘week’] < weeks - 1].groupby(‘dma’)[‘sales’].mean(),,,,,,,,,
,post = df[df[‘week’] == weeks - 1].groupby(‘dma’)[‘sales’].,,,,,,,,,
mean(),,,,,,,,,,
,"did = (post - pre).reset_index().merge(df[[‘dma’, ‘group’]].",,,,,,,,,
"drop_duplicates(), on=’dma’)",,,,,,,,,,
,"t, p = ttest_ind(did[did[‘group’] == ‘T’][‘sales’],",,,,,,,,,
,"did[did[‘group’] == ‘C’][‘sales’],",,,,,,,,,
,equal_var=False),,,,,,,,,
,if p < 0.05:,,,,,,,,,
,significant += 1,,,,,,,,,
,power = significant / n_sim,,,,,,,,,
,"results.append({‘weeks’: weeks, ‘power’: power})",,,,,,,,,
return pd.DataFrame(results),,,,,,,,,,
query = “””,,,,,,,,,,
SELECT,,,,,,,,,,
"customer_zip,",,,,,,,,,,
"transaction_date,",,,,,,,,,,
"sales_amount,-- for volume",,,,,,,,,,
1 AS trans_count-- for counts,,,,,,,,,,
FROM transactions,,,,,,,,,,
WHERE transaction_date BETWEEN ‘2025-03-01’ AND ‘2025-06-13’,,,,,,,,,,
AND customer_zip IS NOT NULL,,,,,,,,,,
“””,,,,,,,,,,
# Execute and load into DataFrame,,,,,,,,,,
"transactions = pd.read_sql(query, engine)",,,,,,,,,,
"[‘dma_code’, pd.Grouper(key=’transaction_date’, freq=’W’)]",,,,,,,,,,
).agg({,,,,,,,,,,
"‘sales_amount’: ‘sum’,# Sum of sales for volume",,,,,,,,,,
‘trans_count’: ‘sum’# Sum of transactions for counts,,,,,,,,,,
}).reset_index(),,,,,,,,,,
EXAMPLE 6: DATA QUALITY CHECKS,,,,,,,,,,
Referenced in: Data Quality Checks section,,,,,,,,,,
# Verify completeness,,,,,,,,,,
coverage = weekly_data.groupby('dma_code').size(),,,,,,,,,,
"print(f""DMAs with full data: {(coverage == expected_weeks).sum()} / 210"")",,,,,,,,,,
# Check for anomalies,,,,,,,,,,
import matplotlib.pyplot as plt,,,,,,,,,,
"weekly_data.groupby('dma_code')['sales_amount'].plot(figsize=(12, 8))",,,,,,,,,,
plt.title('Sales by DMA Over Time'),,,,,,,,,,
if abs(skew) > 1:,,,,,,,,,,
"print(f""Normalized sales skewness: {skew:.2f}"")",,,,,,,,,,
"print(""Consider additional transformations if needed for model",,,,,,,,,,
"assumptions"")",,,,,,,,,,
EXAMPLE 7: PRIMARY ANALYSIS - T-TEST ON NORMALIZED VALUES,,,,,,,,,,
Referenced in: Primary Analysis section,,,,,,,,,,
import pandas as pd,,,,,,,,,,
import numpy as np,,,,,,,,,,
from scipy import stats,,,,,,,,,,
import statsmodels.api as sm,,,,,,,,,,
from statsmodels.stats.anova import anova_lm,,,,,,,,,,
from statsmodels.stats.sandwich_covariance import cov_hc1,,,,,,,,,,
# Load DMA-level pre/post sales and assignment data,,,,,,,,,,
df = pd.read_csv(“geo_rct_results.csv”),,,,,,,,,,
# Step 4: Estimate expected sales using pre-period trend,,,,,,,,,,
# Assumes linear growth and 5 weeks of test period,,,,,,,,,,
analysis_df[‘expected_sales’] = analysis_df[‘pre_avg’] * (1 + analysis_,,,,,,,,,,
df[‘pre_trend’] * 5),,,,,,,,,,
# Step 5: Calculate normalized lift index,,,,,,,,,,
analysis_df[‘lift_index’] = (analysis_df[‘test_avg’] / analysis_,,,,,,,,,,
df[‘expected_sales’]) - 1,,,,,,,,,,
# Step 6: Run OLS regression with treatment group as predictor,,,,,,,,,,
"X = sm.add_constant(pd.get_dummies(analysis_df[‘assignment’], drop_",,,,,,,,,,
"first=True))  # e.g., Control=0, Treatment=1",,,,,,,,,,
y = analysis_df[‘lift_index’],,,,,,,,,,
"model = sm.OLS(y, X).fit()",,,,,,,,,,
print(model.summary()),,,,,,,,,,
# Step 7: Report robust (HC1) standard errors,,,,,,,,,,
robust_cov = cov_hc1(model),,,,,,,,,,
robust_se = np.sqrt(np.diag(robust_cov)),,,,,,,,,,
# Output the DiD interaction coefficient,,,,,,,,,,
interaction_term = ‘assignment[T.Treatment]:post’,,,,,,,,,,
if interaction_term in did_model.params:,,,,,,,,,,
print(f”DiD estimate: {did_model.params[interaction_term]:.4f}”),,,,,,,,,,
else:,,,,,,,,,,
print(“Interaction term not found in model output. Check data,,,,,,,,,,
encoding.”),,,,,,,,,,
EXAMPLE 9: LEAVE-ONE-OUT ANALYSIS,,,,,,,,,,
Referenced in: Robustness Checks section,,,,,,,,,,
import pandas as pd,,,,,,,,,,
import statsmodels.api as sm,,,,,,,,,,
import matplotlib.pyplot as plt,,,,,,,,,,
# Run leave-one-out regression by dropping one DMA at a time,,,,,,,,,,
loo_results = [],,,,,,,,,,
estimate’),,,,,,,,,,
plt.xlabel(‘Excluded DMA’),,,,,,,,,,
plt.ylabel(‘Treatment Effect Estimate’),,,,,,,,,,
plt.title(‘Leave-One-Out Sensitivity Analysis’),,,,,,,,,,
plt.legend(),,,,,,,,,,
plt.tight_layout(),,,,,,,,,,
plt.show(),,,,,,,,,,
EXAMPLE 10: PRE-PERIOD BALANCE CHECK,,,,,,,,,,
Referenced in: Robustness Checks section,,,,,,,,,,
import pandas as pd,,,,,,,,,,
import numpy as np,,,,,,,,,,
import statsmodels.api as sm,,,,,,,,,,
# Create fake pre/test periods using pre-period weeks,,,,,,,,,,
placebo_df = df[df[‘period’] == ‘pre’].copy(),,,,,,,,,,
pivot[‘fake_pre’]) - 1,,,,,,,,,,
# Run placebo regression,,,,,,,,,,
placebo_model = sm.OLS(,,,,,,,,,,
"placebo_pivot[‘fake_lift’],",,,,,,,,,,
"sm.add_constant(pd.get_dummies(placebo_pivot[‘assignment’], drop_",,,,,,,,,,
first=True)),,,,,,,,,,
).fit(),,,,,,,,,,
# Report placebo results,,,,,,,,,,
print(f”Placebo effect: {placebo_model.params[1]:.4f}”),,,,,,,,,,
print(f”Placebo p-value: {placebo_model.pvalues[1]:.4f}”),,,,,,,,,,
EXAMPLE 11: UPDATE MARKETING MIX MODEL WITH EXPERIMENTAL PRIOR,,,,,,,,,,
Referenced in: Feeding Back to Strategy section,,,,,,,,,,
# Example: Updating MMM with experimental prior,,,,,,,,,,
# Prior from experiment: 3.4% ± 1.5%,,,,,,,,,,
from sklearn.cluster import KMeans,,,,,,,,,,
"def stratified_randomization(dmas_df, strat_vars, n_strata=4, seed=42):",,,,,,,,,,
“””,,,,,,,,,,
Stratified randomization for Geo RCT,,,,,,,,,,
“””,,,,,,,,,,
# Standardize stratification variables,,,,,,,,,,
scaler = StandardScaler(),,,,,,,,,,
X = scaler.fit_transform(dmas_df[strat_vars]),,,,,,,,,,
# Create strata using k-means clustering,,,,,,,,,,
"kmeans = KMeans(n_clusters=n_strata, random_state=seed)",,,,,,,,,,
dmas_df[‘stratum’] = kmeans.fit_predict(X),,,,,,,,,,
# Randomize within strata,,,,,,,,,,
np.random.seed(seed),,,,,,,,,,
dmas_df[‘assignment’] = ‘Control’,,,,,,,,,,
for stratum in range(n_strata):,,,,,,,,,,
import pandas as pd,,,,,,,,,,
import numpy as np,,,,,,,,,,
"from joblib import Parallel, delayed",,,,,,,,,,
from scipy.stats import ttest_ind,,,,,,,,,,
"def run_power_simulation(historical_data, effect_sizes=[0.02, 0.03, 0.05],",,,,,,,,,,
"test_weeks=[3, 4, 5, 6, 8], n_sims=1000,",,,,,,,,,,
alpha=0.05):,,,,,,,,,,
“””,,,,,,,,,,
Run power simulation for geographic RCT,,,,,,,,,,
“””,,,,,,,,,,
results = [],,,,,,,,,,
for effect in effect_sizes:,,,,,,,,,,
for weeks in test_weeks:,,,,,,,,,,
# Run simulations in parallel,,,,,,,,,,
sim_results = Parallel(n_jobs=-1)(,,,,,,,,,,
pre_data = historical_data.iloc[start_week - 8:start_week].copy(),,,,,,,,,,
test_data = historical_data.iloc[start_week:start_week + weeks].copy(),,,,,,,,,,
# Random assignment of DMAs to treatment and control,,,,,,,,,,
dmas = historical_data[‘dma_code’].unique(),,,,,,,,,,
"treatment_dmas = np.random.choice(dmas, len(dmas) // 2, replace=False)",,,,,,,,,,
pre_data[‘group’] = pre_data[‘dma_code’].apply(lambda x: ‘T’ if x in,,,,,,,,,,
treatment_dmas else ‘C’),,,,,,,,,,
test_data[‘group’] = test_data[‘dma_code’].apply(lambda x: ‘T’ if x in,,,,,,,,,,
treatment_dmas else ‘C’),,,,,,,,,,
# Aggregate weekly sales by DMA,,,,,,,,,,
pre_avg = pre_data.groupby(‘dma_code’)[‘sales_volume’].mean().reset_,,,,,,,,,,
index(name=’pre_avg’),,,,,,,,,,
test_avg = test_data.groupby(‘dma_code’)[‘sales_volume’].mean().reset_,,,,,,,,,,
index(name=’test_avg’),,,,,,,,,,
# Apply simulated lift to treatment group,,,,,,,,,,
test_avg[‘group’] = test_avg[‘dma_code’].apply(lambda x: ‘T’ if x in,,,,,,,,,,
treatment_dmas else ‘C’),,,,,,,,,,
ANCOVA: Analysis of Covariance; regression,MDE: Minimum Detectable Effect; the smallest,,,,,,,,,
model that includes treatment indicator and,true effect size that an experiment has adequate,,,,,,,,,
continuous covariates,power to detect as statistically significant,,,,,,,,,
Cluster-RCT: Cluster Randomized Controlled,MECE: Mutually Exclusive and Collectively,,,,,,,,,
Trial; experimental design where groups (clusters),Exhaustive; property where categories don't,,,,,,,,,
rather than individuals are randomly assigned to,overlap and cover all possibilities,,,,,,,,,
treatment conditions,MMM: Marketing Mix Model; statistical model,,,,,,,,,
DMA: Designated Market Area; geographic,that decomposes sales into contributions from,,,,,,,,,
regions defined by Nielsen for television,various marketing channels and external factors,,,,,,,,,
"viewership measurement, commonly used as",SMD: Standardized Mean Difference; difference,,,,,,,,,
experimental units in geo tests,"in means divided by pooled standard deviation,",,,,,,,,,
ICC: Intraclass Correlation Coefficient; measures,used to assess balance between groups,,,,,,,,,
the similarity of observations within the same,SUTVA: Stable Unit Treatment Value,,,,,,,,,
cluster relative to observations between clusters,Assumption; assumption that treatment of one,,,,,,,,,
iROAS: Incremental Return on Ad Spend; the,unit doesn't affect outcomes of other units (no,,,,,,,,,
"causal revenue impact per dollar spent, measured",spillover),,,,,,,,,
through experimentation rather than correlation,,,,,,,,,,
ITT: Intent-to-Treat; analysis based on initial,,,,,,,,,,
treatment assignment regardless of actual,,,,,,,,,,
Media Readiness,,,,,,,,,,
□ Platform targeting lists created,Documentation,,,,,,,,,
□ Creative assets approved and identical,□ Results summary drafted,,,,,,,,,
□ Trafficking instructions documented,□ Technical appendix complete,,,,,,,,,
□ Compliance monitoring dashboard live,□ Visualizations created,,,,,,,,,
□ Pacing alerts configured,□ Recommendations clear,,,,,,,,,
,□ Lessons learned captured,,,,,,,,,
Data Infrastructure,,,,,,,,,,
□ KPI data pipeline tested,Action Items,,,,,,,,,
□ ZIP→DMA mapping current,□ Decision rule executed,,,,,,,,,
□ Latency documented and acceptable,□ Budget changes implemented,,,,,,,,,
□ Analysis code templates ready,□ MMM coefficients updated,,,,,,,,,
□ Results template prepared,□ Next test planned,,,,,,,,,
,□ Results socialized,,,,,,,,,
"competitive advantage, offering executive",online forum.,,,,,,,,,
"advising, measurement retooling, and calibration",,,,,,,,,,
of marketing mix models.,,,,,,,,,,
The Central Control team includes ad industry,ACKNOWLEDGEMENTS:,,,,,,,,,
"veterans from Google, DoubleClick, Microsoft,",Various contributors provided valuable input,,,,,,,,,
"Amazon, and Adobe, with deep expertise in","to the production of this paper, namely these",,,,,,,,,
"experimental design, media measurement, data","individuals: John Chandler, PhD, Head of Data",,,,,,,,,
"science, marketing analytics, econometrics, and","Science at Central Control, Managing Partner",,,,,,,,,
applied research.,of Data Insights LLC and Clinical Professor of,,,,,,,,,
Our team has supported 500+ experiments for,"Marketing at the University of Montana, for",,,,,,,,,
"Fortune 500 advertisers, optimizing billions of",help in designing many of these experimental,,,,,,,,,
dollars in business impact.,techniques and technical review of the paper;,,,,,,,,,
,"Kumi Harischandra, research scientist, for",,,,,,,,,
,"technical review of the paper; Campbell Foster,",,,,,,,,,
,"Chief Commercial Office of Central Control, for",,,,,,,,,
,"editing, and Ben Munday, Creative Director of",,,,,,,,,
,"Munday Design, for graphic design.",,,,,,,,,
