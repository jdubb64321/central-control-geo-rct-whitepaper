NORMALIZED WEEKLY SALES INDEX BY DMA,,,
(PRE-PERIOD MEAN =100),,,DMA
120,,,9
115,,,3045
110,,,60
105,,,84
100,,,155
95,,,172
90,,,182196
85,,,199
80,,,END OF
,,,PRE-PERIOD
01-01 01-15 02-01 02-15,1-Mar,15-Mar,
WEEK,,,
▲ Pre-period Normalization  of Weekly Sales by DMA These line plots show raw and normalized weekly sales across 10,,,
"example DMAs. The first panel (Raw Weekly Sales) reveals large differences in baseline market size and week-to-week variance,",,,
obscuring comparability across regions. The second panel (Normalized Weekly Sales Index) adjusts each DMA to its own pre-period,,,
"mean (set to 100), enabling meaningful comparisons of relative change.",,,
Summary statistics confirm the effect of normalization: raw sales had a standard deviation of 263 and a coefficient of variation (CV),,,
"of 0.165, whereas normalized sales had a standard deviation of 4.27 and a CV of 0.042. The sales data shown are actual transaction",,,
records from a Fortune 500 compay. The balance shown between test and control cells – after normalization using each DMA’s,,,
"pre-period trend – is genuine, not synthetic. This example demonstrates why stratification is typically unnecessary when using all",,,
210 DMAs with this normalization method.,,,
provides sufficient statistical power for detecting,Implementation,,
modest lift effects. Certain situations may still,,,
"benefit from more sophisticated approaches,","First, calculate a si",,
such as these:,possible DMA pairs,,
,"sales history, popul",,
,This can be done us,,
STRATIFIED RANDOMIZATION,measures that quan,,
Groups DMAs by key characteristics before,,,
,are across multiple,,
randomizing within strata. (For sample,,,
,each DMA with its,,
"code, see Appendix A, Example 12: Stratified",,,
,"Finally, use a rando",,
Randomization.),,,
,each pair to assign,,
,to control.,,
This guarantees balance on stratification variables,,,
but requires careful selection of stratifying,,,
covariates. Best when you can identify 2-3,COVARIATE-CON,,
"clearly important variables (i.e., with strong",(RE-RANDOMIZA,,
correlation to KPI) and have sufficient DMAs,,,
within each stratum.,,,
,Generates multiple,,
,"(e.g., 10,000 draws",,
Example: Stratify by sales quartile to ensure,pre-specified balan,,
treatment and control arms have equal representation,control over balanc,,
"of high-, medium-, and low-volume markets.",randomization prin,,
MATCHED-PAIR RANDOMIZATION,,,
Pairs similar DMAs based on multiple,,,
"characteristics (e.g., past sales trends, populations,",6 Rerandomizati,,
STRATIFIED RANDOMIZATION,measures that quantify how “far apart” two DMAs,,
Groups DMAs by key characteristics before,,,
,"are across multiple dimensions. Then, match",,
randomizing within strata. (For sample,,,
,each DMA with its most similar partner.,,
"code, see Appendix A, Example 12: Stratified",,,
,"Finally, use a randomization algorithm within",,
Randomization.),,,
,each pair to assign one to treatment and one,,
,to control.,,
This guarantees balance on stratification variables,,,
but requires careful selection of stratifying,,,
covariates. Best when you can identify 2-3,COVARIATE-CONSTRAINED RANDOMIZATION,,
"clearly important variables (i.e., with strong",(RE-RANDOMIZATION),,
correlation to KPI) and have sufficient DMAs,,,
within each stratum.,,,
,Generates multiple random assignments,,
,"(e.g., 10,000 draws) and selects one meeting",,
Example: Stratify by sales quartile to ensure,pre-specified balance criteria. Provides precise,,
treatment and control arms have equal representation,control over balance without sacrificing,,
"of high-, medium-, and low-volume markets.",randomization principles.6,,
MATCHED-PAIR RANDOMIZATION,,,
Pairs similar DMAs based on multiple,,,
"characteristics (e.g., past sales trends, populations,",6 Rerandomization To Improve Covariate,,
POST-STRATIFICATION AND COVARIATE,to lift models.,,
ADJUSTMENT,,,
Proceeds with initial randomization but adjusts,RATIONALE,,
for imbalances in the statistical analysis phase,Confirm that this d,,
through regression or ANCOVA. Doesn’t change,a lift equal to or gre,,
assignment but can recover power lost to,size. Link back to p,,
imbalance.,models to justify th,,
These techniques become particularly valuable,a detectable effect.,,
when:,,,
"• Working with fewer than 210 DMAs (e.g.,",,,
regional tests),MONITORING PL,,
• Unable to run extensive historical simulations,Set up real-time da,,
,ensure that deliver,,
• Stakeholders require demonstrable balance for,flight correction wit,,
confidence,especially in high-s,,
• Effect sizes are expected to be small (<2%),,,
,TREATMENT,,
IMPORTANT DISTINCTION:,VERIFICATI,,
RCT ENHANCEMENTS VS.,,,
QUASI-EXPERIMENTAL METHODS,Establishing robustlaunch ensures that,,
The approaches above are all enhancements,actually reaches tre,,
within the RCT framework – they preserve,out of control DMA,,
randomization while improving efficiency.,valid causal inferen,,
through regression or ANCOVA. Doesn’t change,a lift equal to or greater than the expected effect,,
assignment but can recover power lost to,size. Link back to prior experiments or planning,,
imbalance.,models to justify that the level of spend can drive,,
These techniques become particularly valuable,a detectable effect.,,
when:,,,
"• Working with fewer than 210 DMAs (e.g.,",,,
regional tests),MONITORING PLAN,,
• Unable to run extensive historical simulations,Set up real-time dashboards or pacing reports to,,
,ensure that delivery is on track. Allow for mid-,,
• Stakeholders require demonstrable balance for,"flight correction without altering randomization,",,
confidence,especially in high-stakes or high-budget tests.,,
• Effect sizes are expected to be small (<2%),,,
,TREATMENT-DELIVERY,,
IMPORTANT DISTINCTION:,VERIFICATION,,
RCT ENHANCEMENTS VS.,,,
QUASI-EXPERIMENTAL METHODS,Establishing robust monitoring systems before launch ensures that the intended media exposure,,
The approaches above are all enhancements,actually reaches treatment DMAs while staying,,
within the RCT framework – they preserve,"out of control DMAs, a critical requirement for",,
randomization while improving efficiency.,valid causal inference.,,
,OUTCOMES,,
LATENCY TOLERANCE,While we strongly r,,
"Specify acceptable reporting lag, e.g., “All sales",primary KPI to mai,,
must be posted to the warehouse within five days,"rigor, some tests m",,
of transaction date.” Define a cut-off date for,The critical principl,,
primary analysis to allow for complete data.,must be specified d,,
,added post hoc afte,,
DATA AUDITS,,,
Schedule at least one interim extract during the,,,
,SECONDARY KPI,,
"test to verify record counts, completeness, and",If you must track a,,
DMA-level mapping. Use this as a QA step to,"transactions, avera",,
confirm integrity before final analysis.,"rate), pre-declare w",,
,corrected for multi,,
,Bonferroni) or cons,,
BUSINESS SAFEGUARDS,Remember that eac,,
MINIMUM CONTROLS,increases the risk o,,
"If relevant (e.g., in a cessation test), declare a",,,
minimum media presence in control DMAs to,SUBGROUP ANA,,
avoid business disruption or brand risk.,"Similarly, any plan",,
,high- vs. low-incom,,
EARLY-STOP CRITERIA,should be specified be tested using inte,,
"If conditional stopping is allowed, e.g., to prevent","model, not as separ",,
of transaction date.” Define a cut-off date for,,,
primary analysis to allow for complete data.,"must be specified during the design phase, not",,
,added post hoc after seeing results.,,
DATA AUDITS,,,
Schedule at least one interim extract during the,,,
,SECONDARY KPIS,,
"test to verify record counts, completeness, and","If you must track additional metrics (e.g.,",,
DMA-level mapping. Use this as a QA step to,"transactions, average order value, new-customer",,
confirm integrity before final analysis.,"rate), pre-declare whether p-values will be",,
,"corrected for multiple comparisons (e.g., Holm-",,
,Bonferroni) or considered purely exploratory.,,
BUSINESS SAFEGUARDS,Remember that each additional hypothesis test,,
MINIMUM CONTROLS,increases the risk of false positives.,,
"If relevant (e.g., in a cessation test), declare a",,,
minimum media presence in control DMAs to,SUBGROUP ANALYSIS,,
avoid business disruption or brand risk.,"Similarly, any planned subgroup analyses (e.g.,",,
,"high- vs. low-income DMAs, urban vs. rural)",,
EARLY-STOP CRITERIA,should be specified at the design phase. These will be tested using interaction terms in the primary,,
"If conditional stopping is allowed, e.g., to prevent","model, not as separate experiments. Be cautious:",,
,patterns of varianc,,
The simulation requires the same data format as,idiosyncrasies. The,,
"the actual experiment. (For a code example, see",structure of the pla,,
"Appendix A, Example 2: Power Analysis Data",,,
Structure.),,,
,Step-by-Step Si,,
"For first-party data, this means:","(For sample code, s",,
,13: Power Simulati,,
1. Two years of historical ZIP-level transactions,implementation.),,
2. Same aggregation logic as planned for test,1. Randomly sa,,
period,windows fro,,
3. Same ZIP-to-DMA mapping to ensure,planned 6-wee,,
consistency,of different 6-w,,
,"years, preservi",,
,pre- and post-p,,
For syndicated data:,seasonal conte,,
1. Negotiate access to historical data (ideally,differently than,,
2 years),2. Vary the trea,,
2. Verify no methodology changes during,systematically.,,
this period,"4, 5, 6, 8, 10, a",,
is often the swe3. Account for any markets with incomplete,,,
statistical powehistory,,,
,the actual trade,,
The power simulation will use this exact data,patterns.,,
"structure, preserving real-world variance patterns,",,,
,Step-by-Step Simulation Process:,,
"For first-party data, this means:","(For sample code, see Appendix A, Example",,
,13: Power Simulation Framework for detailed,,
1. Two years of historical ZIP-level transactions,implementation.),,
2. Same aggregation logic as planned for test,1. Randomly sample multiple possible test,,
period,windows from the historical period. For a,,
3. Same ZIP-to-DMA mapping to ensure,"planned 6-week test, we would sample dozens",,
consistency,of different 6-week windows across the two,,
,"years, preserving the calendar structure of",,
,pre- and post-periods. This captures different,,
For syndicated data:,seasonal contexts – a test in Q4 may behave,,
1. Negotiate access to historical data (ideally,differently than Q2.,,
2 years),2. Vary the treatment duration,,
2. Verify no methodology changes during,"systematically. Run parallel simulations at 3,",,
this period,"4, 5, 6, 8, 10, and 12 weeks. While 4-6 weeks",,
is often the sweet spot for balancing cost and3. Account for any markets with incomplete,,,
"statistical power, only simulation revealshistory",,,
,the actual trade-offs for your specific sales,,
The power simulation will use this exact data,patterns.,,
"structure, preserving real-world variance patterns,",,,
,3. Apply the DMA normalization process,,
effect size. This example shows the following assumed parameters: control fraction o,,,
all 210 DMAs; conversions distributed in proportion to population. Green cells: Statist,,,
"Orange to red cells: Increasingly non-significant results (p ≥ 0.05), indicating lower po",,,
• Calculate each DMA’s average weekly sales 7. Test for stati,,,
during its 8-week pre-period pre-specified c,,,
• Compute the week-over-week growth rate 8. Repeat this p,,,
during this baseline with different t,,,
"• Project expected sales during the test period assignments, a",,,
based on this trend empirical distri,,,
• Express actual test-period sales as an index,,,
relative to this projection. This normalization,,,
is crucial: it transforms raw sales (which KEY SIMULATION,,,
vary greatly by DMA size) into comparable,,,
"lift indices, dramatically reducing variance. The Monte Carlo",,,
critical insights:,,,
4. Randomly assign DMAs to treatment,,,
and control groups using the same • Power curves b,,,
randomization logic planned for the probability of det,,,
real experiment (typically equal odds of for each test lengt,,,
assignment to all experiment arms). • Minimum dete,,,
5. Simulate a treatment effect by inflating smallest lift wher,,,
normalized sales values in the treatment • Optimal test con,,,
group during the test period by the minimizes MDE w,,,
"hypothesized effect size (e.g., 3%, 5%). This",,,
creates the “signal” we’re trying to detect,,,
"against the “noise” of natural variation. For example, si",,,
,• Compute the week-over-week growth rate,"8. Repeat this process thousands of times,",
,during this baseline,"with different test windows, treatment",
,• Project expected sales during the test period,"assignments, and durations to build a robust",
,based on this trend,empirical distribution of power outcomes.,
,• Express actual test-period sales as an index,,
,relative to this projection. This normalization,,
,is crucial: it transforms raw sales (which,KEY SIMULATION OUTPUTS,
,vary greatly by DMA size) into comparable,,
,"lift indices, dramatically reducing variance.",The Monte Carlo simulation yields several,
,,critical insights:,
4,Randomly assign DMAs to treatment,,
,and control groups using the same,• Power curves by duration: Shows,
,randomization logic planned for the,"probability of detecting effects at 3%, 5%, 7% lift",
,real experiment (typically equal odds of,for each test length,
,assignment to all experiment arms).,• Minimum detectable effect (MDE): The,
5,Simulate a treatment effect by inflating,smallest lift where power exceeds 80%,
,normalized sales values in the treatment,• Optimal test configuration: The duration that,
,group during the test period by the,minimizes MDE while keeping the test practical,
,"hypothesized effect size (e.g., 3%, 5%). This",,
,creates the “signal” we’re trying to detect,,
,against the “noise” of natural variation.,"For example, simulations might reveal:",
substantial information.,the power target. T,,
• High ICC (> 0.20): Strong within-DMA correl-,the treatment work,,
ation. Adding weeks provides diminishing returns.,"reliably detected, gi",,
,real-world data.,,
A high ICC indicates that data within each DMA,,,
are highly correlated – meaning that more,,,
weeks (or switching to daily data) are needed to,Key decision poi,,
compensate for the lack of independence. Because,1. If power is too lo,,
we treat the number of DMAs as fixed (typically all,• Extend test du,,
"210), we must adjust for ICC through test duration",• Increase medi,,
and data granularity:,• Broaden KPI s,,
,single product,,
Practical Adjustments for High ICC:,• Switch to dail,,
1. Extend the test window to observe more,2. If power is very,,
"time points. If ICC = 0.15, you might need 6",• Consider shor,,
weeks instead of 4 to achieve target power.,• Test more sub,,
,• Run multiple,,
"2. Switch to daily data, if day-to-day variation",of one long tes,,
adds usable signal. Daily data often shows,,,
"lower ICC than weekly aggregates, effectively",3. Document the,,
increasing sample size.,• “5-week test a,,
,3% lift”,,
3. Run parallel simulations with different,• “Daily data i,,
granularities and durations to identify the,to 2.8% vs. we,,
most power-efficient combination. Sometimes,• “Extending be,,
4 weeks of daily data outperforms 6 weeks of,minimal powe,,
weekly data.,,,
weeks (or switching to daily data) are needed to,Key decision points:,,
compensate for the lack of independence. Because,1. If power is too low at business-relevant effect sizes:,,
we treat the number of DMAs as fixed (typically all,• Extend test duration (most common solution),,
"210), we must adjust for ICC through test duration",• Increase media weight to drive larger effects,,
and data granularity:,"• Broaden KPI scope (e.g., full brand line vs.",,
,single product),,
Practical Adjustments for High ICC:,• Switch to daily data if available,,
1. Extend the test window to observe more,2. If power is very high (>95%):,,
"time points. If ICC = 0.15, you might need 6",• Consider shortening the test to save budget,,
weeks instead of 4 to achieve target power.,• Test more subtle tactics or lower spend levels,,
,• Run multiple sequential tests instead,,
"2. Switch to daily data, if day-to-day variation",of one long test,,
adds usable signal. Daily data often shows,,,
"lower ICC than weekly aggregates, effectively",3. Document the final configuration:,,
increasing sample size.,• “5-week test achieves 83% power to detect,,
,3% lift”,,
3. Run parallel simulations with different,• “Daily data improves MDE from 3.5%,,
granularities and durations to identify the,to 2.8% vs. weekly”,,
most power-efficient combination. Sometimes,• “Extending beyond 6 weeks provides,,
4 weeks of daily data outperforms 6 weeks of,minimal power gain”,,
weekly data.,,,
Decision Rule,"If lift ≥3%, increase budget 20%. If <3%, re",,
Primary KPI,"Weekly sales revenue by DMA (from CRM,",,
Experimental Unit,210 US DMAs (Nielsen 2024 definitions),,
Design Type,Multi-armed stepped trial (“Rolling Thunde,,
Randomization,"Simple random assignment, equal probabili",,
Random Seed,42 (stored in repo: geoRCT_Q2_2025_assi,,
Pre-Period,"8 weeks (Mar 1 - Apr 25, 2025)",,
Treatment Period,"5 weeks (Apr 26 - May 30, 2025)",,
Post-Period,"2 weeks (May 31 - Jun 13, 2025) [washout b",,
Treatment Dosage,10M impressions per week @ $10 CPM (≈$,,
Media Channels,"Facebook, Instagram",,
Confidence Level,95% (α = 0.05),,
Power Target,≥80%,,
Simulation Results,84% power to detect 3% lift (via 2 years hist,,
,MDE = 2.7% (minimum detectable effect at,,
Normalization Method,8-week pre-period trend per DMA,,
Analysis Method,T-test  on normalized sales indices,,
Compliance Threshold,Treatment DMAs must receive 90-110% of,,
Exclusions,None (all 210 DMAs included),,
Secondary Analyses,None pre-specified,,
Randomization,"Simple random assignment, equal probability",,
Random Seed,42 (stored in repo: geoRCT_Q2_2025_assignments.csv),,
Pre-Period,"8 weeks (Mar 1 - Apr 25, 2025)",,
Treatment Period,"5 weeks (Apr 26 - May 30, 2025)",,
Post-Period,"2 weeks (May 31 - Jun 13, 2025) [washout buffer]",,
Treatment Dosage,"10M impressions per week @ $10 CPM (≈$500,000 total)",,
Media Channels,"Facebook, Instagram",,
Confidence Level,95% (α = 0.05),,
Power Target,≥80%,,
Simulation Results,84% power to detect 3% lift (via 2 years historical data),,
,MDE = 2.7% (minimum detectable effect at 80% power),,
Normalization Method,8-week pre-period trend per DMA,,
Analysis Method,T-test  on normalized sales indices,,
Compliance Threshold,Treatment DMAs must receive 90-110% of planned spend,,
Exclusions,None (all 210 DMAs included),,
Secondary Analyses,None pre-specified,,
,,leads,
,4,Updated versio,
EXAMPLE RANDOMIZED DMA LIST,,,
"CONTROL TALLAHASSEE-THOMASVILLE, YAKIMA-PASCO-RCHLND-KNNWCK, BAKERSFIELD, BLOOMINGTON, LITTLE ROCK-PINE BLUFF, BEND, OR, TRI-CITIES, TN-VA,",,,
"JOSE, MADISON, PARKERSBURG, HARTFORD-NEW HAVEN, BUTTE-BOZEMAN, SAI",,,
"LYNCHBURG, FRESNO-VISALIA, SYRACUSE, BILLINGS, CHICO-REDDING, ALBU",,,
"GROUP A QUINCY-HANNIBAL-KEOKUK, TULSA, JACKSON, TN, PORTLAND-AUBURN, UTICASPRINGS-PUEBLO, HOUSTON, ALPENA, TOLEDO, SAN DIEGO, MISSOULA, OTTU",,,
"HOLYOKE, HARRISONBURG, GREENSBORO-HIGH POINT-WINSTON SALEM, COLUMB",,,
"LAWTON, WAUSAU-RHINELANDER, WILKES BARRE-SCRANTON, HARRISBURG-LANC",,,
"GROUP B MONROE-EL DORADO, ANCHORAGE, AUGUSTA, SEATTLE-TACOMA, MYRTLE BEACHWORTH, LUBBOCK, LA CROSSE-EAU CLAIRE, CHICAGO, LOUISVILLE, ROCKFOR",,,
"AUSTIN, CLEVELAND-AKRON, RENO, ELMIRA, HELENA, MILWAUKEE, MINOT-BI",,,
"BLUEFIELD-BECKLEY-OAK HILL, LINCOLN-HASTINGS-KEARNEY PLUS",,,
"GROUP C MOBILE-PENSACOLA, JOHNSTOWN-ALTOONA, SPRINGFIELD, MO, TRAVERSE CITWEST PALM BEACH-FT PIERCE, NORTH PLATTE, JOPLIN-PITTSBURG, CLARKSB",,,
"ISLE, GREENVILLE-SPARTANBURG-ASHEVILLE, DENVER, CHARLOTTE, SAN ANT",,,
"MONTEREY-SALINAS, OKLAHOMA CITY, CASPER-RIVERTON, TERRE HAUTE, IDA",,,
"GROUP D BILOXI-GULFPORT, LAKE CHARLES, YOUNGSTOWN, MANKATO, LAREDO, DES MOLANSING, JACKSONVILLE, PANAMA CITY, ATLANTA, BANGOR, CORPUS CHRIST",,,
"PADUCAH-CAPE GIRARDEAU-HARRISBURG, GLENDIVE, ERIE, DETROIT, MEDFOR",,,
"GROUP E EUREKA, PHOENIX, GREAT FALLS, RAPID CITY, SHERMAN-ADA, SALT LAKE CBAY CITY, COLUMBIA, SC, ABILENE-SWEETWATER, CHAMPAIGN-SPRINGFIELD-",,,
"SIOUX FALLS (MITCHELL), GREEN BAY-APPLETON, SIOUX CITY, FAIRBANKS,",,,
"FAY-SPRNGDL, BIRMINGHAM, BOISE, ORLANDO-DAYTONA BEACH-MELBOURNE",,,
"GROUP F HATTIESBURG-LAUREL, BURLINGTON-PLATTSBURGH, LAS VEGAS, ALEXANDRIA,BEAUMONT-PORT ARTHUR, WILMINGTON, BUFFALO, MERIDIAN, GRAND JUNCTIO",,,
"RAPIDS-KALAMAZOO-BATTLE CREEK, SAVANNAH, PITTSBURGH, CEDAR RAPIDS-",,,
"MAR-SAN LUIS OBISPO, SACRAMENTO-STOCKTON-MODESTO, BOWLING GREEN, J",,,
"DAVENPORT-ROCK ISLAND-MOLINE, LAFAYETTE, LA, FORT MYERS-NAPLES, CI",,,
CONTROL,"TALLAHASSEE-THOMASVILLE, YAKIMA-PASCO-RCHLND-KNNWCK, BAKERSFIELD, BALTIMORE, EUGENE, MINNEAPOLIS-SAINT PAUL, PEORIA-BLOOMINGTON, LITTLE ROCK-PINE BLUFF, BEND, OR, TRI-CITIES, TN-VA, WASHINGTON, DC-HAGRSTWN, SAN FRANCISCO-OAKLAND-SAN",,
,"JOSE, MADISON, PARKERSBURG, HARTFORD-NEW HAVEN, BUTTE-BOZEMAN, SAINT LOUIS, SOUTH BEND-ELKHART, AMARILLO, ROANOKE-",,
,"LYNCHBURG, FRESNO-VISALIA, SYRACUSE, BILLINGS, CHICO-REDDING, ALBUQUERQUE-SANTA FE, JUNEAU, TYLER-LONGVIEW (LFKN&NCGD)",,
GROUP A,"QUINCY-HANNIBAL-KEOKUK, TULSA, JACKSON, TN, PORTLAND-AUBURN, UTICA, BATON ROUGE, COLUMBIA-JEFFERSON CITY, LIMA, COLORADO SPRINGS-PUEBLO, HOUSTON, ALPENA, TOLEDO, SAN DIEGO, MISSOULA, OTTUMWA-KIRKSVILLE, ALBANY-SCHENECTADY-TROY, SPRINGFIELD-",,
,"HOLYOKE, HARRISONBURG, GREENSBORO-HIGH POINT-WINSTON SALEM, COLUMBUS-TUPELO-WEST POINT, COLUMBUS, GA, WICHITA FALLS-",,
,"LAWTON, WAUSAU-RHINELANDER, WILKES BARRE-SCRANTON, HARRISBURG-LANCASTER-LEBANON-YORK, HUNTSVILLE-DECATUR-FLORENCE",,
GROUP B,"MONROE-EL DORADO, ANCHORAGE, AUGUSTA, SEATTLE-TACOMA, MYRTLE BEACH-FLORENCE, MIAMI-FORT LAUDERDALE, DALLAS-FORT WORTH, LUBBOCK, LA CROSSE-EAU CLAIRE, CHICAGO, LOUISVILLE, ROCKFORD, ODESSA-MIDLAND, NEW YORK, ROCHESTER-MASON CITY-",,
,"AUSTIN, CLEVELAND-AKRON, RENO, ELMIRA, HELENA, MILWAUKEE, MINOT-BISMARCK-DICKINSON, TWIN FALLS, BOSTON, LAFAYETTE, IN,",,
,"BLUEFIELD-BECKLEY-OAK HILL, LINCOLN-HASTINGS-KEARNEY PLUS",,
GROUP C,"MOBILE-PENSACOLA, JOHNSTOWN-ALTOONA, SPRINGFIELD, MO, TRAVERSE CITY-CADILLAC, NEW ORLEANS, LOS ANGELES, DULUTH-SUPERIOR, WEST PALM BEACH-FT PIERCE, NORTH PLATTE, JOPLIN-PITTSBURG, CLARKSBURG-WESTON, FORT WAYNE, WACO-TEMPLE-BRYAN, PRESQUE",,
,"ISLE, GREENVILLE-SPARTANBURG-ASHEVILLE, DENVER, CHARLOTTE, SAN ANTONIO, PORTLAND, OR, FARGO-VALLEY CITY, AUSTIN,",,
,"MONTEREY-SALINAS, OKLAHOMA CITY, CASPER-RIVERTON, TERRE HAUTE, IDAHO FALLS-POCATELLO",,
GROUP D,"BILOXI-GULFPORT, LAKE CHARLES, YOUNGSTOWN, MANKATO, LAREDO, DES MOINES-AMES, WATERTOWN, OMAHA, MARQUETTE, JONESBORO, LANSING, JACKSONVILLE, PANAMA CITY, ATLANTA, BANGOR, CORPUS CHRISTI, DAYTON, PHILADELPHIA, CHEYENNE-SCOTTSBLUFF,",,
,"PADUCAH-CAPE GIRARDEAU-HARRISBURG, GLENDIVE, ERIE, DETROIT, MEDFORD-KLAMATH FALLS, SHREVEPORT, KANSAS CITY",,
GROUP E,"EUREKA, PHOENIX, GREAT FALLS, RAPID CITY, SHERMAN-ADA, SALT LAKE CITY, CHARLESTON-HUNTINGTON, BINGHAMTON, FLINT-SAGINAW-BAY CITY, COLUMBIA, SC, ABILENE-SWEETWATER, CHAMPAIGN-SPRINGFIELD-DECATUR, KNOXVILLE, SPOKANE, EL PASO, CHATTANOOGA,",,
,"SIOUX FALLS (MITCHELL), GREEN BAY-APPLETON, SIOUX CITY, FAIRBANKS, CHARLESTON, SC, WHEELING-STEUBENVILLE, FORT SMITH-",,
,"FAY-SPRNGDL, BIRMINGHAM, BOISE, ORLANDO-DAYTONA BEACH-MELBOURNE",,
GROUP F,"HATTIESBURG-LAUREL, BURLINGTON-PLATTSBURGH, LAS VEGAS, ALEXANDRIA, LA, DOTHAN, MEMPHIS, RICHMOND-PETERSBURG, MONTGOMERY, BEAUMONT-PORT ARTHUR, WILMINGTON, BUFFALO, MERIDIAN, GRAND JUNCTION-MONTROSE, GREENWOOD-GREENVILLE, LEXINGTON, GRAND",,
,"RAPIDS-KALAMAZOO-BATTLE CREEK, SAVANNAH, PITTSBURGH, CEDAR RAPIDS-WATERLOO-DUBUQUE, MACON, NASHVILLE, SANTA BARBARA-SAN",,
,"MAR-SAN LUIS OBISPO, SACRAMENTO-STOCKTON-MODESTO, BOWLING GREEN, JACKSON, MS, INDIANAPOLIS",,
,"DAVENPORT-ROCK ISLAND-MOLINE, LAFAYETTE, LA, FORT MYERS-NAPLES, CINCINNATI, TUCSON-SIERRA VISTA, HARLINGEN-WESLACO-",,
,(no radius expansion),,
□,Frequency caps consistent across platforms,,
□,Creative assets identical in all DMAs,,
□,Budget distributed across Treatment DMAs,,
,proportional to pre-test KPI rate,,
Data Pipeline Verification:,,,
□,Historical sales data passes integrity checks,,
□,ZIP→DMA mapping uses current DMA,,
,definitions consistent with the medium’s,,
,targeting,,
□,Sales reporting latency documented,,
,and acceptable,,
□,Compliance monitoring dashboard live,,
□,Backup data extraction plan in place,,
proportional to pre-test KPI rate,,,
Data Pipeline Verification:,,,
□ Historical sales data passes integrity checks,,,
□ ZIP→DMA mapping uses current DMA,,,
definitions consistent with the medium’s,,,
targeting,,,
□ Sales reporting latency documented,,,
and acceptable,,,
□ Compliance monitoring dashboard live,,,
□ Backup data extraction plan in place,,,
Tuesday AM : Flag DMAs outside 90-110% of plan,,,
Tuesday PM : Adjust bids/budgets (not targeting),,,
Wednesday : Spot-check creative rotation,,,
Thursday : Review week-to-date pacing,,,
Friday : Send compliance report to stakeholders,,,
CRITICAL RULES:,,,
1. Never change DMA assignments - This breaks randomization,,,
2. Document all adjustments - Include timestamp and rationale,,,
3. Don’t peek at results - Avoid the temptation to check lift mid-flight,,,
was pre-established,,,
"4. Maintain spend ratios - If cutting budget is necessary, due to busin",,,
proportionally across all DMAs,,,
Friday : Send compliance report to stakeholders,,,
CRITICAL RULES:,,,
1. Never change DMA assignments - This breaks randomization,,,
2. Document all adjustments - Include timestamp and rationale,,,
3. Don’t peek at results - Avoid the temptation to check lift mid-flight unless conditional stopping,,,
was pre-established,,,
"4. Maintain spend ratios - If cutting budget is necessary, due to business conditions, cut",,,
proportionally across all DMAs,,,
Problem: Underspending in small DMAs,,,
"• Solution: Increase bids/spend, not targeting",,,
parameters,,,
• Document markets affected and adjustment,,,
dates,,,
Problem: Platform reporting discrepancies,,,
"For example, Facebook Ads Manager shows 1,200",,,
conversions while Google Analytics shows 850 for,,,
the same campaign due to different attribution,,,
windows (view-through vs. click-only) and,,,
tracking methods.,,,
• Solution: Designate one platform as,,,
“source of truth”,,,
• Reconcile differences post-campaign,,,
Problem: Competitive activity spike,,,
• Solution: Continue as planned; document,,,
for post-analysis,,,
• Do not add markets or extend test reactively,,,
dates,,,
Problem: Platform reporting discrepancies,,,
"For example, Facebook Ads Manager shows 1,200",,,
conversions while Google Analytics shows 850 for,,,
the same campaign due to different attribution,,,
windows (view-through vs. click-only) and,,,
tracking methods.,,,
• Solution: Designate one platform as,,,
“source of truth”,,,
• Reconcile differences post-campaign,,,
Problem: Competitive activity spike,,,
• Solution: Continue as planned; document,,,
for post-analysis,,,
• Do not add markets or extend test reactively,,,
,"(For sample code, s",,
For Syndicated Data:,Primary Analysis –,,
Request data pull with these specifications:,,,
• Markets: All 210 US DMAs (or subset used,INTERPRETING R,,
in test),The coefficient on a,,
• Metrics: [Total sales volume / unit sales /,represents the esti,,
transactions],,,
• Time period: [Full date range including pre and,Estimate: 0.0,,
post periods],,,
,Std. Error: 0,,
"• Granularity: Daily (preferred, more statistical",,,
power) or weekly,t value: 2.19,,
Allow 5-7 business days after period close for,p-value: 0.02,,
"transaction settlement and data processing, then",95% CI: [0.00,,
freeze the dataset for analysis.,,,
,This indicates a 3.4,,
DATA QUALITY CHECKS,significant at the 95,,
"If using sales volume (continuous), check",,,
distribution of normalized values.,,,
"For count data, standard t-test typically suffices",,,
"unless counts are very low (< 5 per DMA-week), in",,,
which case consider Poisson regression.,,,
in test),The coefficient on assignment Treatment,,
• Metrics: [Total sales volume / unit sales /,represents the estimated lift. For example:,,
transactions],,,
• Time period: [Full date range including pre and,Estimate: 0.0342,,
post periods],,,
,Std. Error: 0.0156,,
"• Granularity: Daily (preferred, more statistical",,,
power) or weekly,t value: 2.19,,
Allow 5-7 business days after period close for,p-value: 0.029,,
"transaction settlement and data processing, then","95% CI: [0.0036, 0.0648]",,
freeze the dataset for analysis.,,,
,"This indicates a 3.42% lift (p = 0.029), statistically",,
DATA QUALITY CHECKS,significant at the 95% confidence level.,,
"If using sales volume (continuous), check",,,
distribution of normalized values.,,,
"For count data, standard t-test typically suffices",,,
"unless counts are very low (< 5 per DMA-week), in",,,
which case consider Poisson regression.,,,
DIFFERENCE-IN-DIFFERENCES 0.97,-,CONFIRM,
,,COMPARI,
PRE-PERIOD BALANCE CHECK 1.0,1,NO SIGNI,
(T-TEST),,GROUPS,
LEAVE-ONE-OUT (MEAN EFFECT) 3.61,,TREATME,
,,NO SINGL,
DESCRIPTION OF CHECKS,,,
1. Difference-in-Differences (DiD),,,
Compares the change in outcome between Treatment and Control groups f,,,
This provides a cross-check that the estimated lift is not merely due to diff,,,
over time.,,,
2. Pre-Period Balance Check,,,
A simple t-test comparing average pre-treatment outcomes between Treat,,,
"A high p-value (e.g., >0.1) suggests the groups were balanced before treat",,,
of confounding.,,,
3. Leave-One-Out Sensitivity,,,
"Runs the treatment effect estimation multiple times, each time excluding o",,,
(T-TEST),GROUPS WERE WELL BALANCED AT BASELINE.,,
LEAVE-ONE-OUT (MEAN EFFECT) 3.61,TREATMENT EFFECT IS STABLE ACROSS GEOGRAPHIES;,,
,NO SINGLE DMA DRIVES THE RESULT.,,
DESCRIPTION OF CHECKS,,,
1. Difference-in-Differences (DiD),,,
Compares the change in outcome between Treatment and Control groups from pre- to post-period.,,,
This provides a cross-check that the estimated lift is not merely due to different group trajectories,,,
over time.,,,
2. Pre-Period Balance Check,,,
A simple t-test comparing average pre-treatment outcomes between Treatment and Control groups.,,,
"A high p-value (e.g., >0.1) suggests the groups were balanced before treatment, reducing the risk",,,
of confounding.,,,
3. Leave-One-Out Sensitivity,,,
"Runs the treatment effect estimation multiple times, each time excluding one DMA from the",,,
"Treatment group. If the estimated lift remains stable across iterations, the result is not overly",,,
- Test Spend: $3.2MM,,,
- Incremental Revenue: $8.7MM,,,
- iROAS: 2.72,,,
- Recommendation: INCREASE social media budget by 20%,,,
ROBUSTNESS CHECKS:,,,
✓ DiD estimate: 3.38% (consistent),,,
✓ Leave-one-out: All estimates between 2.9% and 3.9%,,,
✓ Pre-period placebo: -0.21% (p = 0.84),,,
✓ Compliance: 96% of Treatment DMAs within range,,,
DATA QUALITY:,,,
- Sales data completeness: 99.7%,,,
- DMA assignment adherence: 100%,,,
- Spillover detected: <2% in 4 Control DMAs,,,
ROBUSTNESS CHECKS:,,,
✓ DiD estimate: 3.38% (consistent),,,
✓ Leave-one-out: All estimates between 2.9% and 3.9%,,,
✓ Pre-period placebo: -0.21% (p = 0.84),,,
✓ Compliance: 96% of Treatment DMAs within range,,,
DATA QUALITY:,,,
- Sales data completeness: 99.7%,,,
- DMA assignment adherence: 100%,,,
- Spillover detected: <2% in 4 Control DMAs,,,
"0 < LIFT < TARGETNOCOLIFT ≤ 0ANYPA",,,
0 < LIFT < TARGET,NO,CO,
LIFT ≤ 0,ANY,PA,
• Is there a frequency threshold?,,,
Design follow-up experiments using the same,,,
rigorous framework.,,,
RECALIBRATING IMPACT RESPONSE CURVE IN MARKETING MI,,,
CHANNEL SPEND,,,
"▲ Illustration provided by MASS Analytics, MMM software pro",,,
Compare the Calibrated Curve with the one previously used in the model.,,,
RECALIBRATING IMPACT RESPONSE CURVE IN MARKETING MIX MODEL,,,
NEW CURVE,,,
OLD CURVE,,,
,RESPONSE CURVE BEFORE RCT (DR 60%),,
,RCT RESULTS,,
,RESPONSE CURVE AFTER RCT (DR 27%),,
CHANNEL SPEND,,,
"▲ Illustration provided by MASS Analytics, MMM software provider.",,,
Compare the Calibrated Curve with the one previously used in the model. Use,,,
smaller markets,,,
Solution:,,,
Solution: Include all DMAs; let randomization Fix delivery issues,,,
handle heterogeneity.,,,
PITFALL: CREATI,,,
PITFALL: PEEKING AT ASSIGNMENTS,,,
Example:,,,
Example:  “Let’s test the new,,,
"“Treatment got more large DMAs, let’s re-",,,
Why it’s wrong:  randomize”,,,
Confounds media e,,,
Why it’s wrong:,,,
Solution:  Not truly a randomized trail when treatment,,,
Keep all non-mediagroups are cherry-picked8,,,
creative testing or v,,,
Solution:  experiment,,,
Design test ahead of time with stratification or,,,
re-randomization with pre-specified criteria,,,
PITFALL: REACTI,,,
PITFALL: VAGUE SUCCESS CRITERIA Example:,,,
“Week 2 looks grea,,,
Example:,,,
Why it’s wrong:  “We’ll see if it works”,,,
Early results are no,,,
Why it’s wrong:,,,
Solution:  Invites post-hoc rationalization,,,
Stick to the plan; sa,,,
Solution:,,,
Define specific lift threshold and decision rule,,,
PITFALL: PEEKING AT ASSIGNMENTS,,,
Example:,,,
Example:  “Let’s test the new creative in Treatment DMAs”,,,
"“Treatment got more large DMAs, let’s re-",,,
Why it’s wrong:  randomize”,,,
Confounds media effect with creative effect,,,
Why it’s wrong:,,,
Solution:  Not truly a randomized trail when treatment,,,
Keep all non-media variables constant; Conduct groups are cherry-picked8,,,
creative testing or versioning as a separate,,,
Solution:  experiment,,,
Design test ahead of time with stratification or,,,
re-randomization with pre-specified criteria,,,
PITFALL: REACTING TO EARLY RESULTS,,,
PITFALL: VAGUE SUCCESS CRITERIA Example:,,,
"“Week 2 looks great, let’s double spending”",,,
Example:,,,
Why it’s wrong:  “We’ll see if it works”,,,
Early results are noisy; changes compromise test,,,
Why it’s wrong:,,,
Solution:  Invites post-hoc rationalization,,,
Stick to the plan; save learnings for next test,,,
Solution:,,,
Define specific lift threshold and decision rule,,,
Why it’s wrong:,,,
Imbalance can masquerade as treatment effect,,,
Solution:,,,
Use normalized metrics or covariate adjustment,,,
ADVANTAGES OF LARGE-SCALE GEOGRAPHIC RANDOMIZED CO,,,
• Scientifically sound: It is a true Randomized Control Trial,,,
"• Omnichannel: It works for cable TV, CTV, digital video, programmat",,,
"home, and more",,,
"• Advertiser-friendly: It is proven to work for advertisers, both large",,,
"• Extensible: Works for various KPIs including sales, foot traffic, TV tu",,,
• Independent: Requires nothing but plan compliance from media fir,,,
partners,,,
"• Low Tech: No special technology required (no clean rooms, user IDs,",,,
"• Privacy assured: No PII, only ZIP codes",,,
• Fraud proof: Performance cannot be gamed: we defy anyone to hack,,,
• Immediate results: Final report in minutes of last KPI data availabil,,,
"• Simple: Easily deployed in any media channel with DMA, ZIP code, o",,,
capabilities,,,
• Generalizable: Projectable to national campaigns because it uses all,,,
ADVANTAGES OF LARGE-SCALE GEOGRAPHIC RANDOMIZED CONTROLLED EXPERIMENT,,,
• Scientifically sound: It is a true Randomized Control Trial,,,
"• Omnichannel: It works for cable TV, CTV, digital video, programmatic, social, search, out-of-",,,
"home, and more",,,
"• Advertiser-friendly: It is proven to work for advertisers, both large and small",,,
"• Extensible: Works for various KPIs including sales, foot traffic, TV tune-in  and more",,,
"• Independent: Requires nothing but plan compliance from media firms, DSPs, agencies or other",,,
partners,,,
"• Low Tech: No special technology required (no clean rooms, user IDs, cookies, etc.)",,,
"• Privacy assured: No PII, only ZIP codes",,,
• Fraud proof: Performance cannot be gamed: we defy anyone to hack it,,,
• Immediate results: Final report in minutes of last KPI data availability,,,
"• Simple: Easily deployed in any media channel with DMA, ZIP code, or other geo targeting",,,
capabilities,,,
• Generalizable: Projectable to national campaigns because it uses all DMAs in the country in the,,,
more than just a methodology. It represents a improvements in e,,,
fundamental shift in how organizations should impact. A 5% impr,,,
approach marketing ROI. While competitors delivers far more v,,,
continue relying on correlation-based attribution in a $1M channel.,,,
models and quasi-experimental methods that systematically work,,,
"systematically overstate digital performance,  mix:",,,
companies that embrace geographic 1. Channel Vali,,,
experimentation gain a decisive edge. In markets channels to est,,,
where share is zero-sum and growth comes  contribution,,,
"at competitors’ expense, understanding what",,,
truly drives incremental sales becomes your  2. Risk Mitigati,,,
"secret weapon, demonstrably changing perception spending witho",,,
of marketing from a cost center  that appear ine,,,
into a growth accelerator. or with matche,,,
driving signific,,,
Opportunity WHILE COMPETITORS CONTINUE 3. underinvested,,,
RELYING ON CORRELATION-BASED and linear TV,,,
"ATTRIBUTION MODELS AND  and CPMs low,",,,
QUASI-EXPERIMENTAL METHODS  returns than ov,,,
THAT SYSTEMATICALLY OVERSTATE 4. Tactical Opti,,,
"DIGITAL PERFORMANCE,  level validation",,,
"COMPANIES THAT EMBRACE variants, audie",,,
GEOGRAPHIC EXPERIMENTATION  strategies,,,
GAIN A DECISIVE EDGE. 5. Continuous,,,
companies that embrace geographic 1.,Channel Validation: Test all major,,
experimentation gain a decisive edge. In markets,channels to establish true incremental,,
where share is zero-sum and growth comes,contribution,,
"at competitors’ expense, understanding what",,,
truly drives incremental sales becomes your  2.,Risk Mitigation: Never cut substantial,,
"secret weapon, demonstrably changing perception",spending without rigorous testing. Channels,,
of marketing from a cost center,that appear ineffective in attribution models,,
into a growth accelerator.,or with matched-market testing may be,,
,driving significant incremental sales,,
Opportunity Discovery: InvestigateWHILE COMPETITORS CONTINUE 3.,"underinvested channels like audio, outdoor,",,
RELYING ON CORRELATION-BASED,and linear TV where attention may be high,,
ATTRIBUTION MODELS AND,"and CPMs low, potentially offering better",,
QUASI-EXPERIMENTAL METHODS,returns than oversaturated digital channels,,
THAT SYSTEMATICALLY OVERSTATE 4.,Tactical Optimization: After channel-,,
"DIGITAL PERFORMANCE,","level validation, test media partners, creative",,
COMPANIES THAT EMBRACE,"variants, audience segments, and messaging",,
GEOGRAPHIC EXPERIMENTATION,strategies,,
GAIN A DECISIVE EDGE. 5.,Continuous Calibration: Track whether,,
,mix changes deliver expected sales impact,,
THE PATH FORWARD,"That said, executin",,
,well requires expert,,
Geographic RCTs offer simplicity in design but,sustained commit,,
profound impact in results. By randomly assigning,needs to build this,,
"regions, delivering media, and measuring",Central Control,,
"outcomes, you cut through the noise of modern","your partner, bri",,
marketing analytics. The careful attention to,methodology an,,
"design details, statistical power, operational",make geographi,,
"excellence, and analytical rigor pays dividends in",and optimal for,,
decision quality.,,,
,Remember: every f,,
This whitepaper demonstrates that while running,prevents wasted sp,,
"experiments requires rigor and attention to detail,",successful test that,,
it’s not as complicated or expensive as many,In the words often,,
"marketers fear. This isn’t rocket science,",“”I have not failed.,,
it’s marketing science.,that won’t work.“,,
The real cost isn’t in the resources required,Start today. Pick yo,,
or the temporary sales suspension in control,a test. Let real scien,,
markets. The real cost is making million-dollar,statistical mysticis,,
media decisions based on flawed measurement,dollar decision. Soo,,
while billions in revenue opportunity hang in the,of leading marketin,,
balance.,"like Netflix, Uber,",,
Every day spent optimizing against inaccurate,leaders and wonder,,
signals is a day your competitors might be using,without this level of,,
better evidence to steal your customers.,,,
"outcomes, you cut through the noise of modern","your partner, bringing battle-tested",,
marketing analytics. The careful attention to,methodology and platform capabilities to,,
"design details, statistical power, operational",make geographic experimentation painless,,
"excellence, and analytical rigor pays dividends in",and optimal for your business.,,
decision quality.,,,
,Remember: every failed experiment that,,
This whitepaper demonstrates that while running,prevents wasted spend is as valuable as a,,
"experiments requires rigor and attention to detail,",successful test that identifies a winning strategy.,,
it’s not as complicated or expensive as many,"In the words often attributed to Thomas Edison,",,
"marketers fear. This isn’t rocket science,","“”I have not failed. I’ve just found 10,000 ways",,
it’s marketing science.,that won’t work.“,,
The real cost isn’t in the resources required,Start today. Pick your largest channel. Design,,
or the temporary sales suspension in control,"a test. Let real science, not assumptions or",,
markets. The real cost is making million-dollar,"statistical mysticism, guide your next million-",,
media decisions based on flawed measurement,"dollar decision. Soon, you’ll be in the company",,
while billions in revenue opportunity hang in the,of leading marketing experimentation experts,,
balance.,"like Netflix, Uber, Airbnb and other market",,
Every day spent optimizing against inaccurate,leaders and wonder how you ever made decisions,,
signals is a day your competitors might be using,without this level of evidence.,,
better evidence to steal your customers.,,,
# Read DMA list,,,
dmas = pd.read_csv(“dma_list.csv”),,,
# Simple random assignment without replacement to avo,,,
"dmas[“arm”] = np.random.choice([“Treatment”, “Control",,,
"size=len(dmas),",,,
replace=False),,,
# Check and print group size balance,,,
group_counts = dmas[“arm”].value_counts(),,,
"print(“Group assignment counts:\n”, group_counts)",,,
# Save assignments,,,
"dmas.to_csv(“geoRCT_assignments.csv”, index=False)",,,
EXAMPLE 2: POWER ANALYSIS DATA STRUCTURE,,,
Referenced in: Historical Data Requirements for Simulation section,,,
# Example data structure for power analysis,,,
historical_data = pd.DataFrame({,,,
"‘dma_code’: [501, 501, 501, ...],",,,
"‘week_ending’: [‘2023-01-07’, ‘2023-01-14’, ...],",,,
"size=len(dmas),",,,
replace=False),,,
# Check and print group size balance,,,
group_counts = dmas[“arm”].value_counts(),,,
"print(“Group assignment counts:\n”, group_counts)",,,
# Save assignments,,,
"dmas.to_csv(“geoRCT_assignments.csv”, index=False)",,,
EXAMPLE 2: POWER ANALYSIS DATA STRUCTURE,,,
Referenced in: Historical Data Requirements for Simulation section,,,
# Example data structure for power analysis,,,
historical_data = pd.DataFrame({,,,
"‘dma_code’: [501, 501, 501, ...],",,,
"‘week_ending’: [‘2023-01-07’, ‘2023-01-14’, ...],",,,
var_total = sd_sales ** 2,,,
var_between = icc * var_total,,,
var_within = var_total - var_between,,,
# Generate random intercepts for each DMA,,,
"dma_effects = np.random.normal(0, np.sqrt(var_bet",,,
# Simulate weekly sales per DMA,,,
data = [],,,
for dma in range(n_dmas):,,,
for week in range(n_weeks):,,,
y = mean_sales + dma_effects[dma] + np.ra,,,
np.sqrt(var_within)),,,
"data.append({‘dma’: dma, ‘week’: week, ‘s",,,
return pd.DataFrame(data),,,
# Estimate ICC using mixed-effects model (robust to i,,,
def estimate_icc(df):,,,
"model = MixedLM.from_formula(‘sales ~ 1’, groups=",,,
result = model.fit(),,,
"var_between = result.cov_re.iloc[0, 0]# Random",,,
"dma_effects = np.random.normal(0, np.sqrt(var_between), size=n_dmas)",,,
# Simulate weekly sales per DMA,,,
data = [],,,
for dma in range(n_dmas):,,,
for week in range(n_weeks):,,,
"y = mean_sales + dma_effects[dma] + np.random.normal(0,",,,
np.sqrt(var_within)),,,
"data.append({‘dma’: dma, ‘week’: week, ‘sales’: y})",,,
return pd.DataFrame(data),,,
# Estimate ICC using mixed-effects model (robust to imbalance),,,
def estimate_icc(df):,,,
"model = MixedLM.from_formula(‘sales ~ 1’, groups=’dma’, data=df)",,,
result = model.fit(),,,
"var_between = result.cov_re.iloc[0, 0]# Random intercept variance",,,
# Apply lift to final week in treatment g,,,
df.loc[(df[‘group’] == ‘T’) & (df[‘week’],,,
‘sales’] *= (1 + lift),,,
# Difference-in-differences by DMA,,,
pre = df[df[‘week’] < weeks - 1].groupby(,,,
post = df[df[‘week’] == weeks - 1].groupb,,,
mean(),,,
did = (post - pre).reset_index().merge(df,,,
"drop_duplicates(), on=’dma’)",,,
"t, p = ttest_ind(did[did[‘group’] == ‘T’]",,,
did[did[‘group’] == ‘C’],,,
equal_var=False),,,
if p < 0.05:,,,
significant += 1,,,
power = significant / n_sim,,,
"results.append({‘weeks’: weeks, ‘power’: powe",,,
,# Difference-in-differences by DMA,,
,pre = df[df[‘week’] < weeks - 1].groupby(‘dma’)[‘sales’].mean(),,
,post = df[df[‘week’] == weeks - 1].groupby(‘dma’)[‘sales’].,,
mean(),,,
,"did = (post - pre).reset_index().merge(df[[‘dma’, ‘group’]].",,
"drop_duplicates(), on=’dma’)",,,
,"t, p = ttest_ind(did[did[‘group’] == ‘T’][‘sales’],",,
,"did[did[‘group’] == ‘C’][‘sales’],",,
,equal_var=False),,
,if p < 0.05:,,
,significant += 1,,
,power = significant / n_sim,,
,"results.append({‘weeks’: weeks, ‘power’: power})",,
return pd.DataFrame(results),,,
# Create database connection (replace with actual cre,,,
engine = create_engine(‘your_connection_string’),,,
# Define SQL query,,,
query = “””,,,
SELECT,,,
"customer_zip,",,,
"transaction_date,",,,
"sales_amount,-- for volume",,,
1 AS trans_count-- for counts,,,
FROM transactions,,,
WHERE transaction_date BETWEEN ‘2025-03-01’ AND ‘2025,,,
AND customer_zip IS NOT NULL,,,
“””,,,
# Execute and load into DataFrame,,,
"transactions = pd.read_sql(query, engine)",,,
query = “””,,,
SELECT,,,
"customer_zip,",,,
"transaction_date,",,,
"sales_amount,-- for volume",,,
1 AS trans_count-- for counts,,,
FROM transactions,,,
WHERE transaction_date BETWEEN ‘2025-03-01’ AND ‘2025-06-13’,,,
AND customer_zip IS NOT NULL,,,
“””,,,
# Execute and load into DataFrame,,,
"transactions = pd.read_sql(query, engine)",,,
transactions[‘transaction_date’] = pd.to_datetime(tra,,,
[‘transaction_date’]),,,
# Aggregate transactions to DMA-week level,,,
weekly_data = transactions.groupby(,,,
"[‘dma_code’, pd.Grouper(key=’transaction_date’, f",,,
).agg({,,,
"‘sales_amount’: ‘sum’,# Sum of sales for volum",,,
‘trans_count’: ‘sum’# Sum of transactions fo,,,
}).reset_index(),,,
EXAMPLE 6: DATA QUALITY CHECKS,,,
Referenced in: Data Quality Checks section,,,
# Verify completeness,,,
coverage = weekly_data.groupby('dma_code').size(),,,
"print(f""DMAs with full data: {(coverage == expected_w",,,
# Check for anomalies,,,
import matplotlib.pyplot as plt,,,
weekly_data.groupby('dma_code')['sales_amount'].plot(,,,
"[‘dma_code’, pd.Grouper(key=’transaction_date’, freq=’W’)]",,,
).agg({,,,
"‘sales_amount’: ‘sum’,# Sum of sales for volume",,,
‘trans_count’: ‘sum’# Sum of transactions for counts,,,
}).reset_index(),,,
EXAMPLE 6: DATA QUALITY CHECKS,,,
Referenced in: Data Quality Checks section,,,
# Verify completeness,,,
coverage = weekly_data.groupby('dma_code').size(),,,
"print(f""DMAs with full data: {(coverage == expected_weeks).sum()} / 210"")",,,
# Check for anomalies,,,
import matplotlib.pyplot as plt,,,
"weekly_data.groupby('dma_code')['sales_amount'].plot(figsize=(12, 8))",,,
plt.title('Sales by DMA Over Time'),,,
weekly_data['sales_index'] = weekly_data.apply(lambda,,,
"baseline_means[x['dma']], axis=1)",,,
# Test normalized values for approximate normality,,,
skew = stats.skew(weekly_data['sales_per_capita']),,,
if abs(skew) > 1:,,,
"print(f""Normalized sales skewness: {skew:.2f}"")",,,
"print(""Consider additional transformations if nee",,,
"assumptions"")",,,
EXAMPLE 7: PRIMARY ANALYSIS - T-TEST ON NORMALIZED VALU,,,
Referenced in: Primary Analysis section,,,
import pandas as pd,,,
import numpy as np,,,
from scipy import stats,,,
import statsmodels.api as sm,,,
from statsmodels.stats.anova import anova_lm,,,
from statsmodels.stats.sandwich_covariance import cov,,,
# Load DMA-level pre/post sales and assignment data,,,
if abs(skew) > 1:,,,
"print(f""Normalized sales skewness: {skew:.2f}"")",,,
"print(""Consider additional transformations if needed for model",,,
"assumptions"")",,,
EXAMPLE 7: PRIMARY ANALYSIS - T-TEST ON NORMALIZED VALUES,,,
Referenced in: Primary Analysis section,,,
import pandas as pd,,,
import numpy as np,,,
from scipy import stats,,,
import statsmodels.api as sm,,,
from statsmodels.stats.anova import anova_lm,,,
from statsmodels.stats.sandwich_covariance import cov_hc1,,,
# Load DMA-level pre/post sales and assignment data,,,
df = pd.read_csv(“geo_rct_results.csv”),,,
analysis_df = analysis_df.merge(,,,
"df[[‘dma_code’, ‘assignment’]].drop_duplicates(),",,,
on=’dma_code’,,,
),,,
# Step 4: Estimate expected sales using pre-period tr,,,
# Assumes linear growth and 5 weeks of test period,,,
analysis_df[‘expected_sales’] = analysis_df[‘pre_avg’,,,
df[‘pre_trend’] * 5),,,
# Step 5: Calculate normalized lift index,,,
analysis_df[‘lift_index’] = (analysis_df[‘test_avg’],,,
df[‘expected_sales’]) - 1,,,
# Step 6: Run OLS regression with treatment group as,,,
X = sm.add_constant(pd.get_dummies(analysis_df[‘assig,,,
"first=True))  # e.g., Control=0, Treatment=1",,,
y = analysis_df[‘lift_index’],,,
"model = sm.OLS(y, X).fit()",,,
print(model.summary()),,,
# Step 7: Report robust (HC1) standard errors,,,
robust_cov = cov_hc1(model),,,
robust_se = np.sqrt(np.diag(robust_cov)),,,
# Step 4: Estimate expected sales using pre-period trend,,,
# Assumes linear growth and 5 weeks of test period,,,
analysis_df[‘expected_sales’] = analysis_df[‘pre_avg’] * (1 + analysis_,,,
df[‘pre_trend’] * 5),,,
# Step 5: Calculate normalized lift index,,,
analysis_df[‘lift_index’] = (analysis_df[‘test_avg’] / analysis_,,,
df[‘expected_sales’]) - 1,,,
# Step 6: Run OLS regression with treatment group as predictor,,,
"X = sm.add_constant(pd.get_dummies(analysis_df[‘assignment’], drop_",,,
"first=True))  # e.g., Control=0, Treatment=1",,,
y = analysis_df[‘lift_index’],,,
"model = sm.OLS(y, X).fit()",,,
print(model.summary()),,,
# Step 7: Report robust (HC1) standard errors,,,
robust_cov = cov_hc1(model),,,
robust_se = np.sqrt(np.diag(robust_cov)),,,
).fit(,,,
"cov_type=’cluster’,",,,
cov_kwds={‘groups’: did_df[‘dma_code’]}  # cluste,,,
),,,
# Output the DiD interaction coefficient,,,
interaction_term = ‘assignment[T.Treatment]:post’,,,
if interaction_term in did_model.params:,,,
print(f”DiD estimate: {did_model.params[interacti,,,
else:,,,
print(“Interaction term not found in model output,,,
encoding.”),,,
EXAMPLE 9: LEAVE-ONE-OUT ANALYSIS,,,
Referenced in: Robustness Checks section,,,
import pandas as pd,,,
import statsmodels.api as sm,,,
import matplotlib.pyplot as plt,,,
# Run leave-one-out regression by dropping one DMA at,,,
loo_results = [],,,
# Output the DiD interaction coefficient,,,
interaction_term = ‘assignment[T.Treatment]:post’,,,
if interaction_term in did_model.params:,,,
print(f”DiD estimate: {did_model.params[interaction_term]:.4f}”),,,
else:,,,
print(“Interaction term not found in model output. Check data,,,
encoding.”),,,
EXAMPLE 9: LEAVE-ONE-OUT ANALYSIS,,,
Referenced in: Robustness Checks section,,,
import pandas as pd,,,
import statsmodels.api as sm,,,
import matplotlib.pyplot as plt,,,
# Run leave-one-out regression by dropping one DMA at a time,,,
loo_results = [],,,
# Plot LOO estimates to identify influential DMAs,,,
"plt.figure(figsize=(10, 6))",,,
"plt.scatter(loo_df[‘excluded_dma’], loo_df[‘estimate’",,,
"plt.axhline(y=0.0342, color=’red’, linestyle=’--’, la",,,
estimate’),,,
plt.xlabel(‘Excluded DMA’),,,
plt.ylabel(‘Treatment Effect Estimate’),,,
plt.title(‘Leave-One-Out Sensitivity Analysis’),,,
plt.legend(),,,
plt.tight_layout(),,,
plt.show(),,,
EXAMPLE 10: PRE-PERIOD BALANCE CHECK,,,
Referenced in: Robustness Checks section,,,
import pandas as pd,,,
import numpy as np,,,
import statsmodels.api as sm,,,
# Create fake pre/test periods using pre-period weeks,,,
placebo_df = df[df[‘period’] == ‘pre’].copy(),,,
estimate’),,,
plt.xlabel(‘Excluded DMA’),,,
plt.ylabel(‘Treatment Effect Estimate’),,,
plt.title(‘Leave-One-Out Sensitivity Analysis’),,,
plt.legend(),,,
plt.tight_layout(),,,
plt.show(),,,
EXAMPLE 10: PRE-PERIOD BALANCE CHECK,,,
Referenced in: Robustness Checks section,,,
import pandas as pd,,,
import numpy as np,,,
import statsmodels.api as sm,,,
# Create fake pre/test periods using pre-period weeks,,,
placebo_df = df[df[‘period’] == ‘pre’].copy(),,,
,on=’dma_code’,,
),,,
# Compute placebo lift (should be ~0 if pre-periods a,,,
placebo_pivot[‘fake_lift’] = (placebo_pivot[‘fake_tes,,,
pivot[‘fake_pre’]) - 1,,,
# Run placebo regression,,,
placebo_model = sm.OLS(,,,
,"placebo_pivot[‘fake_lift’],",,
,sm.add_constant(pd.get_dummies(placebo_pivot[‘ass,,
first=True)),,,
).fit(),,,
# Report placebo results,,,
print(f”Placebo effect: {placebo_model.params[1]:.4f},,,
print(f”Placebo p-value: {placebo_model.pvalues[1]:.4,,,
EXAMPLE 11: UPDATE MARKETING MIX MODEL WITH EXPERIMEN,,,
Referenced in: Feeding Back to Strategy section,,,
# Example: Updating MMM with experimental prior,,,
# Prior from experiment: 3.4% ± 1.5%,,,
pivot[‘fake_pre’]) - 1,,,
# Run placebo regression,,,
placebo_model = sm.OLS(,,,
"placebo_pivot[‘fake_lift’],",,,
"sm.add_constant(pd.get_dummies(placebo_pivot[‘assignment’], drop_",,,
first=True)),,,
).fit(),,,
# Report placebo results,,,
print(f”Placebo effect: {placebo_model.params[1]:.4f}”),,,
print(f”Placebo p-value: {placebo_model.pvalues[1]:.4f}”),,,
EXAMPLE 11: UPDATE MARKETING MIX MODEL WITH EXPERIMENTAL PRIOR,,,
Referenced in: Feeding Back to Strategy section,,,
# Example: Updating MMM with experimental prior,,,
# Prior from experiment: 3.4% ± 1.5%,,,
import pandas as pd,,,
import numpy as np,,,
from sklearn.preprocessing import StandardScaler,,,
from sklearn.cluster import KMeans,,,
"def stratified_randomization(dmas_df, strat_vars, n_s",,,
“””,,,
Stratified randomization for Geo RCT,,,
“””,,,
# Standardize stratification variables,,,
scaler = StandardScaler(),,,
X = scaler.fit_transform(dmas_df[strat_vars]),,,
# Create strata using k-means clustering,,,
"kmeans = KMeans(n_clusters=n_strata, random_state",,,
dmas_df[‘stratum’] = kmeans.fit_predict(X),,,
# Randomize within strata,,,
np.random.seed(seed),,,
dmas_df[‘assignment’] = ‘Control’,,,
for stratum in range(n_strata):,,,
from sklearn.cluster import KMeans,,,
"def stratified_randomization(dmas_df, strat_vars, n_strata=4, seed=42):",,,
“””,,,
Stratified randomization for Geo RCT,,,
“””,,,
# Standardize stratification variables,,,
scaler = StandardScaler(),,,
X = scaler.fit_transform(dmas_df[strat_vars]),,,
# Create strata using k-means clustering,,,
"kmeans = KMeans(n_clusters=n_strata, random_state=seed)",,,
dmas_df[‘stratum’] = kmeans.fit_predict(X),,,
# Randomize within strata,,,
np.random.seed(seed),,,
dmas_df[‘assignment’] = ‘Control’,,,
for stratum in range(n_strata):,,,
"assignments = stratified_randomization(dmas, strat_va",,,
EXAMPLE 13: POWER SIMULATION FRAMEWORK,,,
Referenced in: Simulation-Based Power Estimation section,,,
import pandas as pd,,,
import numpy as np,,,
"from joblib import Parallel, delayed",,,
from scipy.stats import ttest_ind,,,
"def run_power_simulation(historical_data, effect_size",,,
"test_weeks=[3, 4, 5, 6, 8],",,,
alpha=0.05):,,,
“””,,,
Run power simulation for geographic RCT,,,
“””,,,
results = [],,,
for effect in effect_sizes:,,,
for weeks in test_weeks:,,,
# Run simulations in parallel,,,
sim_results = Parallel(n_jobs=-1)(,,,
import pandas as pd,,,
import numpy as np,,,
"from joblib import Parallel, delayed",,,
from scipy.stats import ttest_ind,,,
"def run_power_simulation(historical_data, effect_sizes=[0.02, 0.03, 0.05],",,,
"test_weeks=[3, 4, 5, 6, 8], n_sims=1000,",,,
alpha=0.05):,,,
“””,,,
Run power simulation for geographic RCT,,,
“””,,,
results = [],,,
for effect in effect_sizes:,,,
for weeks in test_weeks:,,,
# Run simulations in parallel,,,
sim_results = Parallel(n_jobs=-1)(,,,
“””,,,
# Sample random test window (ensure space for 8-w,,,
"start_week = np.random.randint(8, len(historical_",,,
# Extract pre and test periods,,,
pre_data = historical_data.iloc[start_week - 8:st,,,
test_data = historical_data.iloc[start_week:start,,,
# Random assignment of DMAs to treatment and cont,,,
dmas = historical_data[‘dma_code’].unique(),,,
"treatment_dmas = np.random.choice(dmas, len(dmas)",,,
pre_data[‘group’] = pre_data[‘dma_code’].apply(la,,,
treatment_dmas else ‘C’),,,
test_data[‘group’] = test_data[‘dma_code’].apply(,,,
treatment_dmas else ‘C’),,,
# Aggregate weekly sales by DMA,,,
pre_avg = pre_data.groupby(‘dma_code’)[‘sales_vol,,,
index(name=’pre_avg’),,,
test_avg = test_data.groupby(‘dma_code’)[‘sales_v,,,
index(name=’test_avg’),,,
# Apply simulated lift to treatment group,,,
test_avg[‘group’] = test_avg[‘dma_code’].apply(la,,,
pre_data = historical_data.iloc[start_week - 8:start_week].copy(),,,
test_data = historical_data.iloc[start_week:start_week + weeks].copy(),,,
# Random assignment of DMAs to treatment and control,,,
dmas = historical_data[‘dma_code’].unique(),,,
"treatment_dmas = np.random.choice(dmas, len(dmas) // 2, replace=False)",,,
pre_data[‘group’] = pre_data[‘dma_code’].apply(lambda x: ‘T’ if x in,,,
treatment_dmas else ‘C’),,,
test_data[‘group’] = test_data[‘dma_code’].apply(lambda x: ‘T’ if x in,,,
treatment_dmas else ‘C’),,,
# Aggregate weekly sales by DMA,,,
pre_avg = pre_data.groupby(‘dma_code’)[‘sales_volume’].mean().reset_,,,
index(name=’pre_avg’),,,
test_avg = test_data.groupby(‘dma_code’)[‘sales_volume’].mean().reset_,,,
index(name=’test_avg’),,,
# Apply simulated lift to treatment group,,,
test_avg[‘group’] = test_avg[‘dma_code’].apply(lambda x: ‘T’ if x in,,,
treatment_dmas else ‘C’),,,
# print(power_results),,,
APPENDIX B: GLOSSARY,,,
ANCOVA: Analysis of Covariance; regression,MDE: Minimum D,,
model that includes treatment indicator and,true effect size that,,
continuous covariates,power to detect as s,,
Cluster-RCT: Cluster Randomized Controlled,MECE: Mutually E,,
Trial; experimental design where groups (clusters),Exhaustive; proper,,
rather than individuals are randomly assigned to,overlap and cover a,,
treatment conditions,MMM: Marketing,,
DMA: Designated Market Area; geographic,that decomposes sa,,
regions defined by Nielsen for television,various marketing,,
"viewership measurement, commonly used as",SMD: Standardize,,
experimental units in geo tests,in means divided b,,
ICC: Intraclass Correlation Coefficient; measures,used to assess bala,,
the similarity of observations within the same,SUTVA: Stable Un,,
cluster relative to observations between clusters,Assumption; assum,,
iROAS: Incremental Return on Ad Spend; the,unit doesn't affect o,,
"causal revenue impact per dollar spent, measured",spillover),,
through experimentation rather than correlation,,,
ITT: Intent-to-Treat; analysis based on initial,,,
ANCOVA: Analysis of Covariance; regression,MDE: Minimum Detectable Effect; the smallest,,
model that includes treatment indicator and,true effect size that an experiment has adequate,,
continuous covariates,power to detect as statistically significant,,
Cluster-RCT: Cluster Randomized Controlled,MECE: Mutually Exclusive and Collectively,,
Trial; experimental design where groups (clusters),Exhaustive; property where categories don't,,
rather than individuals are randomly assigned to,overlap and cover all possibilities,,
treatment conditions,MMM: Marketing Mix Model; statistical model,,
DMA: Designated Market Area; geographic,that decomposes sales into contributions from,,
regions defined by Nielsen for television,various marketing channels and external factors,,
"viewership measurement, commonly used as",SMD: Standardized Mean Difference; difference,,
experimental units in geo tests,"in means divided by pooled standard deviation,",,
ICC: Intraclass Correlation Coefficient; measures,used to assess balance between groups,,
the similarity of observations within the same,SUTVA: Stable Unit Treatment Value,,
cluster relative to observations between clusters,Assumption; assumption that treatment of one,,
iROAS: Incremental Return on Ad Spend; the,unit doesn't affect outcomes of other units (no,,
"causal revenue impact per dollar spent, measured",spillover),,
through experimentation rather than correlation,,,
ITT: Intent-to-Treat; analysis based on initial,,,
treatment assignment regardless of actual,,,
,Randomization code peer-reviewed,,DiD cross-chec
□,DMA assignments generated and locked,□,Leave-one-out
□,Version control established,□,Pre-period plac
,,□,Confidence inte
Media Readiness,,,
□,Platform targeting lists created,Documentation,
□,Creative assets approved and identical,□,Results summa
□,Trafficking instructions documented,□,Technical appe
□,Compliance monitoring dashboard live,□,Visualizations c
□,Pacing alerts configured,□,Recommendati
,,□,Lessons learne
Data Infrastructure,,,
□,KPI data pipeline tested,Action Items,
□,ZIP→DMA mapping current,□,Decision rule e
□,Latency documented and acceptable,□,Budget change
□,Analysis code templates ready,□,MMM coefficie
□,Results template prepared,□,Next test plann
,,□,Results socializ
Media Readiness,,,
□ Platform targeting lists created,Documentation,,
□ Creative assets approved and identical,□ Results summary drafted,,
□ Trafficking instructions documented,□ Technical appendix complete,,
□ Compliance monitoring dashboard live,□ Visualizations created,,
□ Pacing alerts configured,□ Recommendations clear,,
,□ Lessons learned captured,,
Data Infrastructure,,,
□ KPI data pipeline tested,Action Items,,
□ ZIP→DMA mapping current,□ Decision rule executed,,
□ Latency documented and acceptable,□ Budget changes implemented,,
□ Analysis code templates ready,□ MMM coefficients updated,,
□ Results template prepared,□ Next test planned,,
,□ Results socialized,,
"brands, media companies, agencies, and others",research and produ,,
to build a clearer understanding of advertising’s,"Google, Viacom, M",,
true effect on sales and other business outcomes.,Guideline. He is Vi,,
We specialize in helping organizations reframe,sciences industry b,,
how they assess advertising effectiveness for,moderator of the in,,
"competitive advantage, offering executive",online forum.,,
"advising, measurement retooling, and calibration",,,
of marketing mix models.,,,
The Central Control team includes ad industry,ACKNOWLEDGE,,
"veterans from Google, DoubleClick, Microsoft,",Various contributo,,
"Amazon, and Adobe, with deep expertise in",to the production o,,
"experimental design, media measurement, data",individuals: John C,,
"science, marketing analytics, econometrics, and",Science at Central,,
applied research.,of Data Insights LL,,
Our team has supported 500+ experiments for,Marketing at the U,,
"Fortune 500 advertisers, optimizing billions of",help in designing m,,
dollars in business impact.,techniques and tec,,
,Kumi Harischandr,,
,technical review of,,
,Chief Commercial,,
,"editing, and Ben M",,
,"Munday Design, fo",,
"competitive advantage, offering executive",online forum.,,
"advising, measurement retooling, and calibration",,,
of marketing mix models.,,,
The Central Control team includes ad industry,ACKNOWLEDGEMENTS:,,
"veterans from Google, DoubleClick, Microsoft,",Various contributors provided valuable input,,
"Amazon, and Adobe, with deep expertise in","to the production of this paper, namely these",,
"experimental design, media measurement, data","individuals: John Chandler, PhD, Head of Data",,
"science, marketing analytics, econometrics, and","Science at Central Control, Managing Partner",,
applied research.,of Data Insights LLC and Clinical Professor of,,
Our team has supported 500+ experiments for,"Marketing at the University of Montana, for",,
"Fortune 500 advertisers, optimizing billions of",help in designing many of these experimental,,
dollars in business impact.,techniques and technical review of the paper;,,
,"Kumi Harischandra, research scientist, for",,
,"technical review of the paper; Campbell Foster,",,
,"Chief Commercial Office of Central Control, for",,
,"editing, and Ben Munday, Creative Director of",,
,"Munday Design, for graphic design.",,
